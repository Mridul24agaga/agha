{
  "initialUrl": "https://automatetube.com/",
  "initialResearchSummary": "Sure! Let's dive into the content of the \"AutomateTube\" website and break it down in a detailed, conversational, yet professional manner. \n\n### Overview of AutomateTube\n\n**AutomateTube** appears to be an innovative platform focused on AI video generation. The primary offering of this site is a tool that allows users to create videos automatically using artificial intelligence. This is a growing trend in the digital content landscape, as AI-driven solutions can save time and resources for individuals and businesses looking to produce engaging video content without the need for extensive manual editing or video-making skills. \n\n### Main Point\n\nAt its core, AutomateTube is dedicated to simplifying the video creation process. The site aims to cater to various users, ranging from content creators, marketers, businesses, and even hobbyists who want to leverage video as a medium for communication and engagement. \n\nTheir main selling point revolves around the ease and efficiency of generating videos through an automated platform. The site emphasizes that users can create videos quickly and with minimal effort, potentially appealing to those who may not have advanced technical skills or a lot of time to spare. \n\n### Key Offerings\n\n1. **AI Video Generation**: \n   - The platform promotes itself as an AI video generator, suggesting that the technology behind it is designed to create videos based on user inputs or parameters.\n   - This feature likely caters to various video formats and styles, broadening its appeal across several domains, such as marketing videos, educational content, tutorials, and social media clips.\n\n2. **User Accounts**:\n   - There are options for users to **Subscribe** or **Login**, indicating that the platform likely offers personalized user experiences. New users can create accounts to access features, save projects, and manage their video content easily.\n\n3. **Creating Videos**:\n   - The sections labeled **Create Video** and **Create Video Listing** suggest that the platform provides options for both individual video creation and possibly organization or categorization of videos. This could enhance user management of multiple projects.\n\n### Refund Policy\n\nOne of the practical aspects discussed on the website is the **Refund Policy**. Understanding this policy is essential for users considering purchasing services from AutomateTube. Let's break this down:\n\n#### Overview of the Refund Policy\n\nAutomateTube strives for customer satisfaction and has laid out a structured refund policy to manage expectations regarding purchases and service usage. Here’s a detailed analysis of their policies:\n\n- **Eligibility for Refunds**:\n   - Refund requests can be made within **30 days of purchase**. This time frame ensures that users have the opportunity to thoroughly evaluate the service.\n   - The service must not have been fully utilized, which suggests users should be cautious about how much they engage with the tool before seeking a refund.\n   - Acceptable reasons for refunds include:\n     - Technical issues that prevent service usage, showcasing the company’s commitment to quality performance.\n     - The service not meeting advertised specifications, indicating that customer satisfaction with product performance is a priority.\n     - Billing errors, reflecting transparency and accountability in their financial transactions.\n\n- **Refund Process**:\n   - Users need to submit a refund request through their support system. This process likely includes filling out a form or contacting customer service directly.\n   - Users must provide order details and the reason for the refund. This helps streamline the review process by the support team.\n   - A promised review timeline of **48 hours** for refund requests suggests efficient customer service support.\n   - If approved, refunds are processed within **5-7 business days**, allowing users to anticipate when they may receive their funds.\n\n- **Non-Refundable Items**:\n   - The policy clearly outlines items that cannot be refunded, which include:\n     - Completed video projects, highlighting a commitment to client responsibility once a service is rendered.\n     - Used credits or tokens, indicating that once credits are utilized, they are considered spent.\n     - Subscription fees incurred after 30 days of purchase, emphasizing the importance of understanding recurring billing agreements.\n\n### Customer Support\n\nThe website provides a **Contact Us** section for any inquiries related to the refund policy. This is an important aspect, as it shows that they are open to communication, helping customers feel valued and supported. \n\n### Conclusion\n\nIn summary, AutomateTube is a promising platform for anyone looking to generate videos quickly and efficiently using AI technology. Their main focus is on accessibility and user-friendliness, making it an attractive choice for a diverse range of users—from professionals to casual creators. \n\nTheir clear refund policy also enhances trust and transparency, allowing potential users to make informed decisions about their purchases. Overall, AutomateTube seems dedicated to offering a robust video creation experience while prioritizing customer satisfaction through supportive policies and communication. \n\nFor anyone interested in exploring AI-driven video creation, AutomateTube presents a well-structured, user-oriented service worth considering.",
  "researchResults": [
    {
      "url": "https://www.crescendo.ai/news/latest-ai-news-and-updates",
      "content": "Latest AI Breakthroughs and News: Feb-March 2025 | News Crescendo delivers the first Augmented AI solution with PartnerHero acquisition. Read press release. Product Omnichannel AIHuman in the LoopInsights & ReportingApps & IntegrationsDemo Pricing Resources BlogNewsCase StudiesDIY AICompanyCareers Schedule a demo February 15, 2025 Latest AI Breakthroughs and News: Feb-March 2025 & Medha Mehta Wondering what’s happening in the AI world? Here are the latest AI breakthroughs and news that are shaping the world around us! Latest AI Breakthroughs, News, and Updates Wondering what’s happening in the AI world? Here is the list of the latest AI breakthroughs and news you must be aware of. The field of artificial intelligence continues to evolve at an unprecedented pace, with breakthroughs and advancements reshaping industries, governments, and daily life. Here’s a deeper look into some of the most impactful AI-related events. The Fresh and Interesting Artificial Intelligence News Here are all the impactful and latest AI breakthroughs that are shaping the world around us. Google Co-Founder Larry Page Launches AI Startup Dynatomics Date: March 7, 2025 Summary: Larry Page, co-founder of Google, has launched Dynatomics, an AI startup focused on next-generation manufacturing. The company aims to integrate AI-driven automation into industrial production, revolutionizing manufacturing efficiency. Source: Business Today Elon Musk’s AI Chatbot Grok Sparks Controversy Over Trump Claims Date: March 7, 2025 Summary: Elon Musk's AI chatbot, Grok, has sparked controversy after claiming that Donald Trump is a \"Putin-compromised asset.\" The AI-generated response has ignited political debates, raising concerns about bias and misinformation in AI-driven platforms. Source: Times of India AI-Driven Drug \"Rentosertib\" Receives Official Generic Name Date: March 7, 2025 Summary: Insilico Medicine, a leader in AI-driven drug discovery, has received an official generic name approval for its breakthrough drug, Rentosertib. The drug, designed using AI-powered molecular modeling, is set to advance treatments for rare diseases. Source: News-Medical Meta Expands Voice-Powered AI with LLaMA 4 Date: March 7, 2025 Summary: Meta has unveiled LLaMA 4, a new voice-powered AI model designed for natural language interactions. The model aims to improve AI assistants, automated customer service, and real-time translation, making AI-driven communication more seamless. Source: PYMNTS Home Depot Launches AI \"Magic Apron\" for Retail Staff Date: March 7, 2025 Summary: Home Depot has introduced an AI-powered \"Magic Apron\", a generative AI tool that assists retail employees in customer service, inventory management, and DIY recommendations. The wearable AI device is designed to improve efficiency across Home Depot stores. Source: TechXplore Shield AI Raises $240M to Scale AI-Powered Autonomy Platform Date: March 7, 2025 Summary: Shield AI, a defense-tech company specializing in AI-powered autonomy solutions, has raised $240 million at a $5.3 billion valuation. The funding will support the expansion of Hivemind Enterprise, a platform enabling autonomous military aircraft operations. Source: PR Newswire Coeptis AI Partners with Nuburu to Enhance AI and Laser Technology Date: March 7, 2025 Summary: Coeptis AI Affiliates has announced a partnership with Nuburu, a leader in laser-based AI applications, to develop next-generation AI-enhanced manufacturing technologies. This collaboration aims to drive innovation in automated laser processing. Source: StockTitan Cornell University Receives $10.5 Million for AI Research Date: March 7, 2025 Summary: Cornell University has received a $10.5 million donation to fund AI-related research. The grant aims to advance machine learning, robotics, and AI ethics, positioning Cornell as a leader in AI-driven academic innovation. Source: Shiksha Microsoft Develops AI Reasoning Models to Compete with OpenAI Date: March 7, 2025 Summary: Microsoft is reportedly working on AI reasoning models to rival OpenAI’s advanced models. These new AI systems focus on improved logical reasoning and decision-making, positioning Microsoft as a major competitor in general AI development. Source: Reuters Perplexity AI Launches $50 Million Venture Fund for AI Startups Date: March 6, 2025 Summary: AI search startup Perplexity is launching a $50 million venture fund to support early-stage AI startups. The fund, backed by prominent investors, aims to accelerate AI-driven innovation by providing capital and resources to emerging companies in the sector. Source: PYMNTS AI Screening May Impact Foreign Student Visas in the U.S. Date: March 6, 2025 Summary: The U.S. government is reportedly using AI-driven screening methods to assess foreign student visa applications, with a focus on detecting potential pro-Palestinian activism. The new system has raised concerns about bias and freedom of expression, sparking debates on AI’s role in immigration policies. Source: The Guardian Meta Targets Businesses with \"Agentic AI\" Initiative Date: March 6, 2025 Summary: Meta has announced an ambitious plan to integrate Agentic AI into business operations, aiming to reach hundreds of millions of businesses worldwide. This initiative will allow enterprises to automate workflows and enhance customer interactions using advanced AI models. Source: CNBC AI Dubbing Startup Dubformer Raises $3.6 Million Date: March 6, 2025 Summary: Dubformer, an AI-powered dubbing startup, has secured $3.6 million in funding from ex-Microsoft and Netflix executives. The startup leverages AI to provide seamless dubbing for media content, catering to a global audience by reducing language barriers in entertainment and education. Source: Tech Funding News AI Research Assistant Startup Conveo Secures $5.3 Million Date: March 6, 2025 Summary: Conveo, a Belgian AI startup developing an AI-powered research coworker, has raised $5.3 million to expand its operations in the U.S. and Europe. The platform aims to assist researchers with data analysis, literature reviews, and content summarization using generative AI. Source: Tech Funding News Peer-to-Peer AI Network FortyTwo Raises $2.3 Million Date: March 6, 2025 Summary: FortyTwo, a decentralized peer-to-peer AI network, has raised $2.3 million to compete with OpenAI, Anthropic, and DeepSeek. The startup seeks to democratize AI by enabling users to contribute computing resources and participate in AI model development. Source: Tech Funding News WHO Establishes AI Governance Collaborating Center for Health Date: March 6, 2025 Summary: The World Health Organization (WHO) has announced a new AI for Health Governance center to oversee ethical AI development in healthcare. The initiative focuses on ensuring transparency, fairness, and accountability in AI applications for global health services. Source: WHO McDonald's Implements AI to Enhance Operations Date: March 6, 2025 Summary: McDonald's is deploying artificial intelligence across its 43,000 locations to improve service speed and reduce employee stress. The upgrades include smart kitchen equipment, AI-enabled drive-throughs, and predictive maintenance systems to enhance the overall customer experience. Source: New York Post BBC News Establishes AI Department for Personalized Content Date: March 6, 2025 Summary: BBC News is creating a new department, BBC News Growth, Innovation, and AI, to integrate AI technology for tailoring content to individual audience preferences. This initiative aims to engage younger audiences who consume news via smartphones and social media platforms. Source: The Guardian Google Search Introduces AI Mode for Complex Questions Date: March 5, 2025 Summary: Google has launched an AI-powered search mode that allows users to ask multi-part, complex questions. The update enhances search capabilities by providing contextual and conversational responses, improving the user experience for in-depth queries. Source: TechCrunch AI Pioneers Andrew Barto and Richard Sutton Win 2025 Turing Award Date: March 5, 2025 Summary: Andrew Barto and Richard Sutton, known for their groundbreaking work in reinforcement learning, have been awarded the prestigious 2025 Turing Award. Their contributions have shaped modern AI techniques used in robotics, game theory, and autonomous systems. Source: The New York Times Amazon Prime Video Introduces AI Dubbing for Global Expansion Date: March 5, 2025 Summary: Amazon Prime Video has rolled out an AI-powered dubbing feature to break language barriers. This technology enables seamless multilingual content translation, allowing users worldwide to access entertainment in their native languages. Source: Deadline Meta Platforms Overhauls Content Moderation Policies Date: March 3, 2025 Summary: Meta, led by CEO Mark Zuckerberg, has implemented significant policy changes, including replacing third-party fact-checking with community notes and relaxing hate speech policies. These changes aim to promote open discussions but have sparked criticism from various communities. Source: Business Insider Anthropic CEO Predicts Superintelligent AI by Next Year Date: March 2, 2025 Summary: Dario Amodei, CEO of Anthropic, predicts that superintelligent AI, surpassing all human capabilities, could emerge as soon as next year. He emphasizes the need for new societal structures, such as universal basic income, to adapt to this transformative technology. Source: The Times Amazon Introduces Alexa Plus with Advanced AI Features Date: March 1, 2025 Summary: Amazon has launched Alexa Plus, an ambitious new version of its virtual assistant. Leveraging advanced AI technology, Alexa Plus aims to provide a more intuitive and interactive user experience, positioning Amazon competitively in the virtual assistant market. Source: The Verge Google's Sergey Brin Advocates 60-Hour Workweeks for AI Teams Date: February 28, 2025 Summary: Google co-founder Sergey Brin has urged employees working on artificial intelligence to spend at least 60 hours per week in the office. He believes this commitment is crucial for Google to succeed in the competitive race to develop artificial general intelligence (AGI). Source: New York Post ‍OpenAI Unveils GPT-4.5, Its Most Advanced AI Model Date: February 27, 2025 Summary: OpenAI has announced GPT-4.5, its largest and most advanced AI model to date. This release marks a significant milestone in natural language processing, offering enhanced capabilities and understanding compared to its predecessors. Source: TechCrunch Infosys Launches Open-Source 'Responsible AI' Toolkit Date: February 26, 2025 Summary: Infosys has introduced an open-source \"Responsible AI\" toolkit as part of its Infosys Topaz Responsible AI Suite. This toolkit aims to identify and prevent security risks, privacy violations, biased outputs, and other potential issues in AI applications, promoting ethical AI use across industries. Source: TechCircle Apple Announces $500 Billion U.S. Investment Plan Including AI Initiatives Date: February 24, 2025 Summary: Apple has unveiled a $500 billion investment plan in the United States over the next four years. The plan includes building an AI server factory in Texas and creating 20,000 research and development jobs, underscoring Apple's commitment to advancing AI technologies. Source: The Guardian Humane Shuts Down AI Pin and Sells Assets to HP Date: February 21, 2025 Summary: AI startup Humane announced it is discontinuing its AI Pin device and selling parts of its business to HP for $116 million. Existing AI Pins will function until February 28, 2025, after which cloud-dependent features will cease. Refunds are available for recent purchasers. Source: Business Insider Florida State University Launches AI@FSU Initiative Date: February 15, 2025 Summary: Florida State University has unveiled AI@FSU, a comprehensive initiative designed to integrate artificial intelligence into teaching, learning, research, and campus experiences. The program provides the university community with access to the latest AI tools, projects, and news through a dedicated website. This effort aims to equip students, faculty, and staff with the skills necessary to effectively and ethically utilize AI technologies. Source: Mirage News YouTube Enhances Shorts with AI-Generated Video Clips Date: February 14, 2025 Summary: YouTube has introduced new generative AI features for its Shorts platform, allowing creators to generate video clips using text prompts. Powered by Google DeepMind's Veo 2 model, this update enables users to create standalone video content or enhance existing Shorts with AI-generated visuals. The feature is currently available in select countries, with plans for broader expansion. Source: Movieguide The Guardian Partners with OpenAI to Integrate Journalism into ChatGPT Date: February 14, 2025 Summary: Guardian Media Group has announced a strategic partnership with OpenAI to incorporate The Guardian's journalism into ChatGPT's platform. This collaboration will make The Guardian's reporting and archived content accessible to ChatGPT's global users through attributed summaries and article extracts. The initiative aims to expand The Guardian's reach and impact, while OpenAI seeks to enhance the ChatGPT experience with real-time news content. Source: Engadget Interview Kickstart Launches Generative AI Course Date: February 14, 2025 Summary: Interview Kickstart has introduced a new Generative AI (GenAI) course designed for professionals aiming to enhance their skills in artificial intelligence. The course offers a comprehensive curriculum covering the latest GenAI concepts and applications, structured to accommodate busy schedules. Participants will engage in live classes led by active machine learning professionals, gaining practical experience in deploying generative AI models across various industries. Source: Yahoo Finance Baidu to Open-Source Latest Ernie AI Model Date: February 14, 2025 Summary: Chinese tech giant Baidu has announced plans to make its next-generation artificial intelligence model, Ernie, open-source by June 30, 2025. This strategic shift comes as competition intensifies, particularly from startups like DeepSeek, which offer open-source AI services. In addition to open-sourcing Ernie, Baidu will make its AI chatbot, Ernie Bot, freely available starting April 1. These initiatives aim to accelerate AI adoption and expand Baidu's market presence in the evolving AI landscape. Source: Reuters Google Forms AI Partnership with Poland Date: February 13, 2025 Summary: Google has signed a memorandum of understanding with Poland to accelerate the adoption of Artificial Intelligence (AI) across various sectors, including cybersecurity, health, and energy. CEO Sundar Pichai announced the partnership, emphasizing its potential to drive economic growth and transform key industries in Poland. This collaboration reflects Google's commitment to leveraging AI for societal and economic advancements. Source: Reuters German Firm to Supply 6,000 AI-Enabled Drones to Ukraine Date: February 13, 2025 Summary: German defense company Helsing has announced plans to manufacture 6,000 AI-enabled HX-2 strike drones for Ukraine. The HX-2 drones, capable of traveling 100 kilometers and operating in swarms, are designed to resist electronic warfare measures. Helsing has also completed its first mass-production facility in southern Germany, with an initial monthly capacity of over 1,000 drones. This initiative aims to enhance Ukraine's surveillance and strike capabilities amid ongoing conflicts. Source: The Defense Post Elon Musk Unveils Grok 3, Claims AI Superiority Date: February 13, 2025 Summary: Elon Musk has announced the upcoming release of Grok 3, the latest AI chatbot from his startup xAI. Musk claims that Grok 3 outperforms all existing AI chatbots, including OpenAI's ChatGPT, highlighting its advanced reasoning capabilities. The chatbot is expected to be released by the end of the month. This development underscores Musk's continued influence in the AI sector and his commitment to advancing AI technology. Source: Yahoo Finance OpenAI Removes Content Warnings from ChatGPT Date: February 13, 2025 Summary: OpenAI has updated its ChatGPT platform by removing certain warning messages that previously indicated when content might violate its usage policies. This change aims to provide a smoother user experience by reducing interruptions during interactions. The company emphasizes its commitment to balancing user engagement with responsible AI use, ensuring that the platform remains both user-friendly and aligned with ethical standards. Source: TechCrunch Adobe Introduces AI-Powered Video Generation Tools Date: February 12, 2025 Summary: Adobe has unveiled its new artificial intelligence (AI) video generation offerings, allowing subscribers to create five-second videos using text prompts. The service is available through Adobe's Firefly application, which integrates with Creative Cloud and offers plans priced at $9.99 and $29.99 per month. This move positions Adobe in direct competition with AI video tools from companies like OpenAI. Source: pymnts.com UK Criminalizes AI-Generated Child Abuse Material Date: February 1, 2025 Summary: The UK government has introduced new legislation making it a criminal offense to use artificial intelligence tools to generate child abuse material. The move follows growing concerns over AI’s potential misuse in creating realistic and harmful content. Law enforcement agencies will be granted enhanced powers to track and prosecute offenders leveraging AI for illegal activities. The legislation is part of a broader effort to regulate AI-generated content, ensuring that emerging technologies are not exploited for criminal purposes. Experts say this law could set a precedent for other nations seeking to curb the dangers of AI misuse. Source: Reuters AI-Powered Digital Therapeutics Transform Neurocare for Parkinson’s Date: February 1, 2025 Summary: Researchers and healthcare providers are leveraging AI-powered digital therapeutics to revolutionize neurocare for Parkinson’s disease. These AI-driven tools analyze patient data in real time, offering personalized treatment plans, symptom tracking, and predictive insights to improve disease management. Unlike traditional methods, AI-powered therapeutics can adapt dynamically to each patient’s needs, enhancing mobility and quality of life. Experts believe this innovation will significantly reduce the burden on healthcare systems while empowering patients to take a more active role in managing their condition. Clinical trials are already underway, showing promising results in improving motor function and cognitive health. Source: ICT & Health X Enhances Grok AI with Image Editing Capabilities Using Aurora Model Date: February 1, 2025 Summary: Elon Musk’s social media platform, X, has announced an upgrade to its AI chatbot, Grok, introducing advanced image editing features powered by the Aurora model. This enhancement allows users to generate, modify, and refine images directly within the chat interface. The update aligns with X’s broader push to integrate AI across its ecosystem, making content creation more accessible to users. Analysts believe this move positions X as a competitor in the generative AI space, rivaling offerings from OpenAI’s DALL·E and Adobe’s AI tools. The update is expected to roll out in phases, with early access available to premium users. Source: TestingCatalog NASA Blocks China's DeepSeek Date: January 31, 2025 Summary: NASA has officially blocked China's AI company, DeepSeek, from its systems, following similar actions taken by other federal agencies. The decision is driven by growing concerns over national security and the potential risks posed by foreign AI models accessing sensitive U.S. government data. The move aligns with broader U.S. efforts to limit China's influence in AI research and development, particularly in areas with potential military or cybersecurity implications. While DeepSeek has rapidly gained prominence for its advanced reasoning capabilities, U.S. officials remain wary of its data handling and potential ties to the Chinese government. Source: CNBC OpenAI Valued at $340 Billion in New Investment Round Date: January 30, 2025 Summary: OpenAI is reportedly in discussions for a new investment round that could value the company at up to $340 billion, making it one of the most valuable AI firms in the world. This latest funding push underscores growing investor confidence in OpenAI’s continued dominance in artificial intelligence. The valuation reflects strong demand for AI models like ChatGPT, which have seen widespread adoption across industries. Analysts suggest that OpenAI may use the funds to further improve AI efficiency, expand cloud infrastructure, and develop more advanced multimodal models capable of reasoning across text, image, and audio inputs. Source: Reuters Alibaba Releases AI Model Surpassing DeepSeek-V3 Date: January 29, 2025 Summary: Alibaba has unveiled its latest AI model, claiming it outperforms DeepSeek-V3, one of the most advanced reasoning models on the market. The new model represents Alibaba’s ongoing push to lead China’s AI race against U.S. tech giants like OpenAI and Google DeepMind. With a focus on improved efficiency and accuracy, Alibaba's AI system aims to cater to enterprise applications, including AI-driven business analytics and automated customer support. The release comes amid increasing competition in China’s AI sector, where companies are striving to develop domestic alternatives to Western AI technologies. Source: Reuters OpenAI Partners with U.S. National Laboratories for Nuclear Security Research Date: January 28, 2025 Summary: OpenAI has entered a strategic partnership with U.S. National Laboratories to advance AI-driven research in nuclear security. The collaboration focuses on leveraging artificial intelligence to enhance threat detection, improve response systems, and secure critical infrastructure. As concerns grow over AI’s potential misuse in cybersecurity and warfare, this partnership aims to ensure AI remains a tool for national defense. The initiative is part of a broader government push to integrate AI in key security sectors while maintaining strict oversight on its development and deployment in high-stakes environments. Source: NBC News DeepSeek’s AI App Triggers U.S. Tech Stock Sell-Off Date: January 27, 2025 Summary: The release of DeepSeek’s latest AI application has caused a sharp decline in U.S. tech stocks, as investors fear increased competition from China’s rapidly advancing AI sector. The app, designed for complex problem-solving and data analysis, has demonstrated capabilities that rival leading U.S. AI models. Analysts believe the sell-off reflects broader concerns over China's growing AI influence and its potential impact on U.S. tech dominance. The development has renewed discussions on AI regulations, export controls, and the need for continued investment in domestic AI research to maintain a competitive edge. Source: BBC News DeepSeek's AI Model Challenges U.S. Tech Dominance Date: January 27, 2025 Summary: Chinese startup DeepSeek unveiled n AI model that rivals leading U.S. technologies but operates with significantly fewer resources. This development led to a substantial decline in U.S. tech stocks, with Nvidia's valuation dropping by nearly $600 billion. The emergence of DeepSeek's cost-effective AI solutions has raised concerns about the future of U.S. leadership in artificial intelligence. Source: MarketWatch Nvidia's Market Valuation Drops Amid AI Competition Date: January 27, 2025 Summary: Following the emergence of DeepSeek's competitive AI model, Nvidia experienced a significant decline in market valuation, losing nearly $600 billion. This event marked the largest one-day loss in market capitalization for a U.S. company in history. The development has prompted discussions about the evolving dynamics of the AI industry and the challenges faced by established tech giants. Source: CNBC DeepSeek's AI Assistant Tops App Store Charts Date: January 27, 2025 Summary: DeepSeek's AI Assistant surpassed ChatGPT to become the highest-rated free app on the U.S. App Store. The chatbot demonstrated capabilities in answering questions, solving logic problems, and writing computer programs, performing on par with existing market leaders. Its efficient use of resources and rapid development have positioned DeepSeek as a formidable competitor in the AI landscape. Source: The Guardian Photobucket to License Public User Photos for AI Training Date: January 25, 2025 Summary: Photobucket, a major image-hosting service, has announced plans to license publicly available user photos to AI companies for training datasets. The move has raised privacy concerns among users, as many were unaware their content could be used for AI development. Photobucket clarified that only publicly marked images would be included, and private content would remain excluded. However, users who wish to opt-out must manually set their accounts to private. The decision reflects a broader trend where AI firms seek access to large image datasets to improve computer vision models and image-generation AI. Source: Business Insider OpenAI Launches o3-mini, Its Latest AI Reasoning Model Date: January 24, 2025 Summary: OpenAI has introduced o3-mini, the newest model in its \"o\" series, designed to improve AI reasoning and efficiency. Positioned as a “smart, fast model,” the o3-mini is optimized for both consumer and enterprise use, offering enhanced reasoning capabilities while maintaining lower computational costs. OpenAI has made the model available through both the ChatGPT interface and API access, allowing developers to integrate it into their own applications. The release comes amid increasing demand for smaller, more efficient AI models that can perform well across a wide range of reasoning and decision-making tasks. Source: The Economic Times Meta's $65 Billion Investment in AI and New Data Center Date: January 24, 2025 Summary: Meta CEO Mark Zuckerberg announced plans to invest up to $65 billion in artificial intelligence throughout 2025. A significant portion of this investment is allocated for the completion of a major AI data center in Louisiana. This facility is expected to support Meta's AI initiatives, including the development of the Llama large language model, and underscores the company's commitment to advancing AI technologies. Source: NYTimes OpenAI Introduces 'Operator' AI Assistant Date: January 24, 2025 Summary: OpenAI launched \"Operator,\" a new AI assistant capable of handling various online tasks, such as ordering groceries and processing ticket purchases. Initially available to ChatGPT Pro users, the Operator functions as a semi-autonomous agent, requiring user input for specific actions like account logins. The assistant emphasizes user safety by restricting access to harmful websites and ensuring ethical operation. Source: MarketingProfs SoftBank in Talks to Lead $500 Million AI Funding Round Date: January 23, 2025 Summary: SoftBank is reportedly in discussions to lead a $500 million funding round for an undisclosed AI startup. The investment reflects SoftBank’s continued interest in artificial intelligence, following its past backing of companies like OpenAI and Anthropic. The firm’s AI-focused Vision Fund has been instrumental in supporting the growth of key players in the AI industry. Analysts speculate that the funding may go toward a next-generation AI company focused on foundational models, robotics, or AI-powered enterprise solutions. If finalized, this funding round would further cement SoftBank’s influence in shaping the future of AI technology. Source: Yahoo Finance Databricks Closes $15.3 Billion Financing at $62 Billion Valuation Date: January 22, 2025 Summary: Databricks, a leader in data analytics and AI infrastructure, has secured $15.3 billion in new financing, raising its valuation to $62 billion. The funding round includes participation from major investors, with Meta joining as a strategic partner. Databricks specializes in providing AI-powered data management solutions and has been a key player in enabling enterprises to leverage large-scale data for machine learning applications. The investment signals continued strong demand for AI infrastructure and data analytics solutions, as companies seek to enhance AI capabilities with robust, scalable data management platforms. Source: TechCrunch Samsung Unveils Next-Generation AI Features at Galaxy Unpacked Date: January 22, 2025 Summary: At its Galaxy Unpacked event in San Jose, Samsung Electronics introduced advanced AI features for its upcoming Galaxy S series. The new capabilities aim to provide more natural and intuitive user experiences, integrating AI seamlessly into daily activities. This development signifies Samsung's commitment to enhancing mobile AI applications and setting new industry standards. Source: Samsung Newsroom Elon Musk Criticizes 'Stargate Project' Funding Claims Date: January 22, 2025 Summary: Elon Musk, serving as a White House adviser, expressed skepticism about the recently announced \"Stargate Project,\" an AI initiative involving OpenAI, SoftBank, Oracle, and MGX. Musk alleged that the claimed $100 billion immediate investment and the projected $500 billion over four years were not genuinely secured, suggesting that SoftBank had less than $10 billion committed. This critique led to a public exchange with OpenAI's Sam Altman, who defended the project's legitimacy and potential benefits. Source: Business Today President Trump Announces $500 Billion AI Infrastructure Initiative Date: January 21, 2025 Summary: In collaboration with OpenAI, Oracle, and SoftBank, President Trump announced the launch of \"The Stargate Project,\" an ambitious plan to invest up to $500 billion in AI infrastructure across the United States. The initiative aims to establish data centers and bolster AI capabilities, with an initial commitment of $100 billion and plans to scale up over the next four years. Source: Reuters CES 2025 Highlights AI-Powered Innovations Date: January 7, 2025 Summary: The Consumer Electronics Show (CES) 2025 in Las Vegas showcased a plethora of AI-powered robots, gadgets, and next-generation technologies. Notably, X CEO Linda Yaccarino introduced \"Trend Genius,\" an AI-driven tool designed to enhance advertising campaigns by leveraging trending topics. The event underscored the growing integration of AI across various consumer products and industries. Source: New York Post OpenAI's \"12 Days of OpenAI\" Event Unveils Major AI Advancements Summary: OpenAI hosted its \"12 Days of OpenAI\" event, introducing several significant features and models. Notably, they launched \"Sora,\" a text-to-video model enabling users to create realistic videos from text prompts, available to ChatGPT Plus and Pro users. Additionally, OpenAI released the \"o1\" model, designed for advanced reasoning through chain-of-thought processing, enhancing its ability to tackle complex tasks with improved accuracy and transparency. The event also marked the debut of \"ChatGPT Pro,\" a subscription service priced at $200 per month, offering unlimited access to the o1 model and enhanced voice features. Source: OpenAI Google Releases Gemini 2.0 and New AI Generation Models Summary: Google announced significant AI advancements, including the release of Gemini 2.0, their most capable model to date, featuring agentic capabilities for developers, enterprises, and individuals. They also introduced new AI video and image generation models, Veo 2 and Imagen 3, achieving state-of-the-art results in head-to-head comparisons. These developments highlight Google's commitment to advancing AI technology across various applications. Source: Google AI Blog Nvidia Finalizes $700 Million Acquisition of Run:ai Summary: Nvidia completed its $700 million acquisition of Israeli AI firm Run:ai after overcoming regulatory scrutiny. The European Commission granted unconditional approval, determining that the deal posed no significant competitive risks. Run:ai assists developers in optimizing AI infrastructure and plans to make its software open-source, expanding compatibility beyond Nvidia GPUs to the broader AI ecosystem. Source: Reuters AI-Powered Cameras Tested to Detect Drunk Drivers in the UK Summary: In a pioneering initiative, AI-powered cameras capable of detecting drunk drivers were tested in Devon and Cornwall, UK. Developed by Australian firm Acusensus, the system analyzes images of passing vehicles to identify signs of driver impairment and alerts nearby police for immediate intervention. This technology aims to enhance road safety by providing law enforcement with advanced tools to monitor and address dangerous driving behaviors. Source: BBC AI Judge to Score Usyk vs. Fury Rematch Summary: The rematch between boxers Oleksandr Usyk and Tyson Fury in Riyadh, Saudi Arabia, featured an AI-powered fourth judge to score the bout. Introduced by Saudi boxing chief Turki Alalshikh, the AI judge aimed to offer unbiased and error-free scoring, though it did not impact the official results. This experiment represents a significant step toward integrating AI into sports officiating to enhance fairness and accuracy. Source: TalkSport OpenAI's o1 Model Demonstrates Advanced Reasoning Capabilities Summary: OpenAI released the o1 model, designed to engage in explicit reasoning through chain-of-thought processing. This approach enables the model to tackle complex tasks with improved accuracy and transparency, marking a significant advancement in AI's ability to perform advanced reasoning and problem-solving. Source: OpenAI Google's Major AI Advancements Announced in December 2024 Summary: Google capped off 2024 with several major announcements showcasing its progress in artificial intelligence. Among the highlights were Gemini 2.0, a cutting-edge AI model with agentic capabilities designed for developers, enterprises, and individuals. The company also introduced Veo 2 and Imagen 3, state-of-the-art models for AI-driven video and image generation, setting new benchmarks in creative applications. These developments reinforce Google's leadership in advancing AI technologies across diverse domains. Source: Google AI Blog Amazon Invests an Additional $4 Billion in Anthropic Summary: Amazon announced a further $4 billion investment in AI startup Anthropic, doubling its previous commitment. This move strengthens Amazon's position in the competitive AI landscape, particularly against rivals like Microsoft and Google. The investment will support Anthropic's development of advanced AI models and enhance its use of Amazon Web Services (AWS) infrastructure. Source: Amazon U.S. Convenes International Network to Address AI Safety Summary: The United States hosted the inaugural meeting of the International Network of AI Safety Institutes (AISIs) in San Francisco, bringing together experts from nine countries and the European Commission to tackle AI safety risks. The network aims to develop a global understanding of AI safety, evaluate AI models, and mitigate risks such as cybersecurity threats and misuse for malicious purposes. Source: Time OpenAI Buys Chat.com Domain Summary: OpenAI secured the Chat.com domain, redirecting it to ChatGPT, to enhance user accessibility and brand visibility. This strategic acquisition signals a focus on mainstream adoption. Source: TechCrunch U.S. Hosts Global AI Safety Network Meeting Summary: The U.S. launched the International Network of AI Safety Institutes, uniting experts from nine nations to tackle AI risks like cybersecurity and misuse, underscoring global collaboration in AI governance. Source: Reuters Mistral AI Enhances Le Chat with Flux Pro Summary: Mistral AI upgraded its Le Chat chatbot by integrating Flux Pro, an advanced image generation model. This update boosts the chatbot’s creative capabilities for a richer user experience. Source: TechCrunch We're building the world's best customer experience platform. Crescendo has joined forces with PartnerHero to launch an advanced suite of customer experience services, powered by Augmented AI. Get the Brief About Crescendo Crescendo developed the world’s first augmented-AI customer experience (CX) platform that combines the power of AI with human domain expertise to deliver outcome-driven results for fast-moving, digitally-native enterprises growing quickly. With its unique approach, Crescendo offers scalable solutions that optimize both frontline and back office operations, helping companies create meaningful, personalized customer experiences. ‍ Media Contact Lonn Johnston for Crescendo lonn@flak42.com +1.650.219.7764 ‍ Keep Reading AI News ### The Latest VC Investment Deals in AI Startups - 2025 In 2025, the artificial intelligence (AI) sector witnessed several significant venture capital (VC) investment deals globally. Here are some massive VC deals in AI startups. February 15, 2025 AI News ### Latest Customer Service Industry News: 2025 Stay updated on the latest customer service industry news, AI advancements, layoffs, and CX trends from top companies like Intercom, Shopify, AT&T, and more. February 4, 2025 AI News ### 20 New AI Technologies, Breakthroughs, Innovations — 2025 Discover 20 groundbreaking AI technologies transforming business, healthcare, content creation & more in 2025. Stay ahead with the latest innovations. February 3, 2025 201 Spear Street San Francisco, CA 94105 Email: info@crescendo.aiPhone: +1 (888) 410-8077 Product Omnichannel AIHuman in the LoopInsights & ReportingApps & IntegrationsDemoPricing RESOURCES CompanyNewsBlogCase StudiesDIY AI LEGAL Terms of UsePrivacy PolicyCookie Policy Your Privacy Choices Notice at Collection Sign up for our newsletter. Get valuable CX strategies, product announcements, and tips on upcoming events straight to your inbox. Thanks, your submission has been received. Oops! Something went wrong, try again. © 2025 Crescendo. All rights reserved. Try our voice assistant. This is a sample of Crescendo’s voice assistant technology. Take it for a spin. End Mute",
      "title": "https://www.crescendo.ai/news/latest-ai-news-and-updates"
    },
    {
      "url": "https://opentools.ai/news/freepik-and-google-unleash-veo-2-the-future-of-ai-video-generation-is-here",
      "content": "Veo 2 - Google DeepMind Jump to Content Google DeepMind Search... Search Close Google DeepMind About Learn about Google DeepMind — Our mission is to build AI responsibly to benefit humanity Responsibility & Safety — We want AI to benefit the world, so we must be thoughtful about how it’s built and used Education — Our vision is to help make the AI ecosystem more representative of society Careers — Many disciplines, one common goal Research View Research — We work on some of the most complex and interesting challenges in AI. Breakthroughs — Explore some of the biggest innovations in AI Publications — Explore a selection of our recent research Technologies View Technologies — Solving the world’s most complex challenges Gemini — The most general and capable AI models we've ever built Project Astra — A universal AI agent that is helpful in everyday life Imagen — Our highest quality text-to-image model Veo — Our state-of-the-art video generation model Overview Veo 2 (New) AlphaFold — Accelerating breakthroughs in biology with AI Overview Impact stories AlphaFold Server AlphaFold Database SynthID — Identifying AI-generated content Discover View Discover — Discover our latest breakthroughs and see how we’re shaping the future Blog — Discover our latest AI breakthroughs, projects, and updates Events — Meet our team and learn more about our research The Podcast — Uncover the extraordinary ways AI is transforming our world Search... Search Close Learn about Google DeepMind Responsibility & Safety — We want AI to benefit the world, so we must be thoughtful about how it’s built and used Education — Our vision is to help make the AI ecosystem more representative of society Careers — Many disciplines, one common goal Latest posts Start building with Gemini 2.0 Flash and Flash-Lite 25 February 2025 Gemini 2.0 is now available to everyone 5 February 2025 View Research Breakthroughs — Explore some of the biggest innovations in AI Publications — Explore a selection of our recent research Latest research posts Google DeepMind at NeurIPS 2024 5 December 2024 Genie 2: A large-scale foundation world model 4 December 2024 View Technologies Gemini — The most general and capable AI models we've ever built Project Astra — A universal AI agent that is helpful in everyday life Imagen — Our highest quality text-to-image model Veo — Our state-of-the-art video generation model AlphaFold — Accelerating breakthroughs in biology with AI SynthID — Identifying AI-generated content Latest technology posts Start building with Gemini 2.0 Flash and Flash-Lite 25 February 2025 Gemini 2.0 is now available to everyone 5 February 2025 Veo Overview Veo 2 (New) View Discover Blog — Discover our latest AI breakthroughs, projects, and updates Events — Meet our team and learn more about our research The Podcast — Uncover the extraordinary ways AI is transforming our world Latest posts Start building with Gemini 2.0 Flash and Flash-Lite 25 February 2025 Gemini 2.0 is now available to everyone 5 February 2025 Veo 2 Our state-of-the-art video generation model Try on VideoFX Try on Vertex AI Pause video Play video Veo creates videos with realistic motion and high quality output, up to 4K. Explore different styles and find your own with extensive camera controls. Veo creates videos with realistic motion and high quality output, up to 4K. Explore different styles and find your own with extensive camera controls. Capabilities Benchmarks Limitations Redefining quality and control Veo 2 is able to faithfully follow simple and complex instructions, and convincingly simulates real-world physics as well as a wide range of visual styles. Enhanced realism and fidelity Significantly improves over other AI video models in terms of detail, realism, and artifact reduction. Advanced motion capabilities Veo represents motion to a high degree of accuracy, thanks to its understanding of physics and its ability to follow detailed instructions. Greater camera control options Interprets instructions precisely to create a wide range of shot styles, angles, movements – and combinations of all of these. Pause video Play video Prompt: An extreme close-up shot focuses on the face of a female DJ, her beautiful, voluminous black curly hair framing her features as she becomes completely absorbed in the music. Her eyes are closed, lost in the rhythm, and a slight smile plays on her lips. The camera captures the subtle movements of her head as she nods and sways to the beat, her body instinctively responding to the music pulsating through her headphones and out into the crowd. The shallow depth of field blurs the background. She’s surrounded by vibrant neon colors. The close-up emphasizes her captivating presence and the power of music to transport and transcend. - + Prompt: An extreme close-up shot focuses on the face of a female DJ, her beautiful, voluminous black curly hair framing her features as she becomes completely absorbed in the music. Her eyes are closed, lost in the rhythm, and a slight smile plays on her lips. The camera captures the subtle movements of her head as she nods and sways to the beat, her body instinctively responding to the music pulsating through her headphones and out into the crowd. The shallow depth of field blurs the background. She’s surrounded by vibrant neon colors. The close-up emphasizes her captivating presence and the power of music to transport and transcend. Prompt: An extreme close-up shot focuses on the face of a female DJ, her beautiful, voluminous black curly hair framing her features as she becomes completely absorbed in the music. Her eyes are closed, lost in the rhythm, and a slight smile plays on her lips. The camera captures the subtle movements of her head as she nods and sways to the beat, her body instinctively responding to the music pulsating through her headphones and out into the crowd. The shallow depth of field blurs the background. She’s surrounded by vibrant neon colors. The close-up emphasizes her captivating presence and the power of music to transport and transcend. Pause video Play video Prompt: This medium shot, with a shallow depth of field, portrays a cute cartoon girl with wavy brown hair, sitting upright in a 1980s kitchen. Her hair is medium length and wavy. She has a small, slightly upturned nose, and small, rounded ears. She is very animated and excited as she talks to the camera. - + Prompt: This medium shot, with a shallow depth of field, portrays a cute cartoon girl with wavy brown hair, sitting upright in a 1980s kitchen. Her hair is medium length and wavy. She has a small, slightly upturned nose, and small, rounded ears. She is very animated and excited as she talks to the camera. Prompt: This medium shot, with a shallow depth of field, portrays a cute cartoon girl with wavy brown hair, sitting upright in a 1980s kitchen. Her hair is medium length and wavy. She has a small, slightly upturned nose, and small, rounded ears. She is very animated and excited as she talks to the camera. View on YouTube Pause video Play video Prompt: The sun rises slowly behind a perfectly plated breakfast scene. Thick, golden maple syrup pours in slow motion over a stack of fluffy pancakes, each one releasing a soft, warm steam cloud. A close-up of crispy bacon sizzles, sending tiny embers of golden grease into the air. Coffee pours in smooth, swirling motion into a crystal-clear cup, filling it with deep brown layers of crema. Scene ends with a camera swoop into a fresh-cut orange, revealing its bright, juicy segments in stunning macro detail. - + Prompt: The sun rises slowly behind a perfectly plated breakfast scene. Thick, golden maple syrup pours in slow motion over a stack of fluffy pancakes, each one releasing a soft, warm steam cloud. A close-up of crispy bacon sizzles, sending tiny embers of golden grease into the air. Coffee pours in smooth, swirling motion into a crystal-clear cup, filling it with deep brown layers of crema. Scene ends with a camera swoop into a fresh-cut orange, revealing its bright, juicy segments in stunning macro detail. Prompt: The sun rises slowly behind a perfectly plated breakfast scene. Thick, golden maple syrup pours in slow motion over a stack of fluffy pancakes, each one releasing a soft, warm steam cloud. A close-up of crispy bacon sizzles, sending tiny embers of golden grease into the air. Coffee pours in smooth, swirling motion into a crystal-clear cup, filling it with deep brown layers of crema. Scene ends with a camera swoop into a fresh-cut orange, revealing its bright, juicy segments in stunning macro detail. View on YouTube Pause video Play video Prompt: The camera floats gently through rows of pastel-painted wooden beehives, buzzing honeybees gliding in and out of frame. The motion settles on the refined farmer standing at the center, his pristine white beekeeping suit gleaming in the golden afternoon light. He lifts a jar of honey, tilting it slightly to catch the light. Behind him, tall sunflowers sway rhythmically in the breeze, their petals glowing in the warm sunlight. The camera tilts upward to reveal a retro farmhouse with mint-green shutters, its walls dappled with shadows from swaying trees. Shot with a 35mm lens on Kodak Portra 400 film, the golden light creates rich textures on the farmer’s gloves, marmalade jar, and weathered wood of the beehives. - + Prompt: The camera floats gently through rows of pastel-painted wooden beehives, buzzing honeybees gliding in and out of frame. The motion settles on the refined farmer standing at the center, his pristine white beekeeping suit gleaming in the golden afternoon light. He lifts a jar of honey, tilting it slightly to catch the light. Behind him, tall sunflowers sway rhythmically in the breeze, their petals glowing in the warm sunlight. The camera tilts upward to reveal a retro farmhouse with mint-green shutters, its walls dappled with shadows from swaying trees. Shot with a 35mm lens on Kodak Portra 400 film, the golden light creates rich textures on the farmer’s gloves, marmalade jar, and weathered wood of the beehives. Prompt: The camera floats gently through rows of pastel-painted wooden beehives, buzzing honeybees gliding in and out of frame. The motion settles on the refined farmer standing at the center, his pristine white beekeeping suit gleaming in the golden afternoon light. He lifts a jar of honey, tilting it slightly to catch the light. Behind him, tall sunflowers sway rhythmically in the breeze, their petals glowing in the warm sunlight. The camera tilts upward to reveal a retro farmhouse with mint-green shutters, its walls dappled with shadows from swaying trees. Shot with a 35mm lens on Kodak Portra 400 film, the golden light creates rich textures on the farmer’s gloves, marmalade jar, and weathered wood of the beehives. View on YouTube Veo 2 outperforms other leading video generation models, based on human evaluations of its performance. Veo 2 outperforms other leading video generation models, based on human evaluations of its performance. Pause video Play video Prompt: Low-angle tracking shot, 18mm lens. The car drifts, leaving trails of light and tire smoke, creating a visually striking and abstract composition. The camera tracks low, capturing the sleek, olive green muscle car as it approaches a corner. As the car executes a dramatic drift, the shot becomes more stylized. The spinning wheels and billowing tire smoke, illuminated by the surrounding city lights and lens flare, create streaks of light and color against the dark asphalt. The cityscape – yellow cabs, neon signs, and pedestrians – becomes a blurred, abstract backdrop. Volumetric lighting adds depth and atmosphere, transforming the scene into a visually striking composition of motion, light, and urban energy. - + Prompt: Low-angle tracking shot, 18mm lens. The car drifts, leaving trails of light and tire smoke, creating a visually striking and abstract composition. The camera tracks low, capturing the sleek, olive green muscle car as it approaches a corner. As the car executes a dramatic drift, the shot becomes more stylized. The spinning wheels and billowing tire smoke, illuminated by the surrounding city lights and lens flare, create streaks of light and color against the dark asphalt. The cityscape – yellow cabs, neon signs, and pedestrians – becomes a blurred, abstract backdrop. Volumetric lighting adds depth and atmosphere, transforming the scene into a visually striking composition of motion, light, and urban energy. Prompt: Low-angle tracking shot, 18mm lens. The car drifts, leaving trails of light and tire smoke, creating a visually striking and abstract composition. The camera tracks low, capturing the sleek, olive green muscle car as it approaches a corner. As the car executes a dramatic drift, the shot becomes more stylized. The spinning wheels and billowing tire smoke, illuminated by the surrounding city lights and lens flare, create streaks of light and color against the dark asphalt. The cityscape – yellow cabs, neon signs, and pedestrians – becomes a blurred, abstract backdrop. Volumetric lighting adds depth and atmosphere, transforming the scene into a visually striking composition of motion, light, and urban energy. Pause video Play video Prompt: Cinematic shot of a female doctor in a dark yellow hazmat suit, illuminated by the harsh fluorescent light of a laboratory. The camera slowly zooms in on her face, panning gently to emphasize the worry and anxiety etched across her brow. She is hunched over a lab table, peering intently into a microscope, her gloved hands carefully adjusting the focus. The muted color palette of the scene, dominated by the sickly yellow of the suit and the sterile steel of the lab, underscores the gravity of the situation and the weight of the unknown she is facing. The shallow depth of field focuses on the fear in her eyes, reflecting the immense pressure and responsibility she bears. - + Prompt: Cinematic shot of a female doctor in a dark yellow hazmat suit, illuminated by the harsh fluorescent light of a laboratory. The camera slowly zooms in on her face, panning gently to emphasize the worry and anxiety etched across her brow. She is hunched over a lab table, peering intently into a microscope, her gloved hands carefully adjusting the focus. The muted color palette of the scene, dominated by the sickly yellow of the suit and the sterile steel of the lab, underscores the gravity of the situation and the weight of the unknown she is facing. The shallow depth of field focuses on the fear in her eyes, reflecting the immense pressure and responsibility she bears. Prompt: Cinematic shot of a female doctor in a dark yellow hazmat suit, illuminated by the harsh fluorescent light of a laboratory. The camera slowly zooms in on her face, panning gently to emphasize the worry and anxiety etched across her brow. She is hunched over a lab table, peering intently into a microscope, her gloved hands carefully adjusting the focus. The muted color palette of the scene, dominated by the sickly yellow of the suit and the sterile steel of the lab, underscores the gravity of the situation and the weight of the unknown she is facing. The shallow depth of field focuses on the fear in her eyes, reflecting the immense pressure and responsibility she bears. Pause video Play video Prompt: The camera spirals down through an infinite network of glowing threads, pulsating with multicolored light. The setting feels alive, each thread thrumming with faint whispers and bursts of imagery—fractals, mythological beasts, and celestial maps. The courier darts through the maze, their silhouette painted with the kaleidoscopic glow of the fibers. As they weave between strands, their every touch triggers animations—one a glowing phoenix, another a blooming lotus—until they stumble upon a massive, golden thread. It flares, and a holographic figure emerges: a younger version of themselves, surrounded by fiery glyphs. The scene shifts between soft, glowing pastels and brilliant, fiery tones, blending hand-drawn 2D animation with dynamic light effects, captured in fluid, sweeping motion. - + Prompt: The camera spirals down through an infinite network of glowing threads, pulsating with multicolored light. The setting feels alive, each thread thrumming with faint whispers and bursts of imagery—fractals, mythological beasts, and celestial maps. The courier darts through the maze, their silhouette painted with the kaleidoscopic glow of the fibers. As they weave between strands, their every touch triggers animations—one a glowing phoenix, another a blooming lotus—until they stumble upon a massive, golden thread. It flares, and a holographic figure emerges: a younger version of themselves, surrounded by fiery glyphs. The scene shifts between soft, glowing pastels and brilliant, fiery tones, blending hand-drawn 2D animation with dynamic light effects, captured in fluid, sweeping motion. Prompt: The camera spirals down through an infinite network of glowing threads, pulsating with multicolored light. The setting feels alive, each thread thrumming with faint whispers and bursts of imagery—fractals, mythological beasts, and celestial maps. The courier darts through the maze, their silhouette painted with the kaleidoscopic glow of the fibers. As they weave between strands, their every touch triggers animations—one a glowing phoenix, another a blooming lotus—until they stumble upon a massive, golden thread. It flares, and a holographic figure emerges: a younger version of themselves, surrounded by fiery glyphs. The scene shifts between soft, glowing pastels and brilliant, fiery tones, blending hand-drawn 2D animation with dynamic light effects, captured in fluid, sweeping motion. Benchmarks Veo has achieved state of the art results in head-to-head comparisons of outputs by human raters over top video generation models. Participants viewed 1003 prompts and respective videos on MovieGenBench, a benchmark dataset released by Meta. Veo 2 performs best on overall preference, and for its capability to follow prompts accurately. All comparisons were done at 720p resolution. Veo sample duration is 8s, VideoGen’s sample duration is 10s, and other models' durations are 5s. We show the full video duration to raters. Pause video Play video Prompt: A low-angle shot captures a flock of pink flamingos gracefully wading in a lush, tranquil lagoon. The vibrant pink of their plumage contrasts beautifully with the verdant green of the surrounding vegetation and the crystal-clear turquoise water. Sunlight glints off the water's surface, creating shimmering reflections that dance on the flamingos' feathers. The birds' elegant, curved necks are submerged as they walk through the shallow water, their movements creating gentle ripples that spread across the lagoon. The composition emphasizes the serenity and natural beauty of the scene, highlighting the delicate balance of the ecosystem and the inherent grace of these magnificent birds. The soft, diffused light of early morning bathes the entire scene in a warm, ethereal glow. - + Prompt: A low-angle shot captures a flock of pink flamingos gracefully wading in a lush, tranquil lagoon. The vibrant pink of their plumage contrasts beautifully with the verdant green of the surrounding vegetation and the crystal-clear turquoise water. Sunlight glints off the water's surface, creating shimmering reflections that dance on the flamingos' feathers. The birds' elegant, curved necks are submerged as they walk through the shallow water, their movements creating gentle ripples that spread across the lagoon. The composition emphasizes the serenity and natural beauty of the scene, highlighting the delicate balance of the ecosystem and the inherent grace of these magnificent birds. The soft, diffused light of early morning bathes the entire scene in a warm, ethereal glow. Prompt: A low-angle shot captures a flock of pink flamingos gracefully wading in a lush, tranquil lagoon. The vibrant pink of their plumage contrasts beautifully with the verdant green of the surrounding vegetation and the crystal-clear turquoise water. Sunlight glints off the water's surface, creating shimmering reflections that dance on the flamingos' feathers. The birds' elegant, curved necks are submerged as they walk through the shallow water, their movements creating gentle ripples that spread across the lagoon. The composition emphasizes the serenity and natural beauty of the scene, highlighting the delicate balance of the ecosystem and the inherent grace of these magnificent birds. The soft, diffused light of early morning bathes the entire scene in a warm, ethereal glow. View on YouTube Pause video Play video Prompt: A cinematic shot captures a fluffy Cockapoo, perched atop a vibrant pink flamingo float, in a sun-drenched Los Angeles swimming pool. The crystal-clear water sparkles under the bright California sun, reflecting the playful scene. The Cockapoo's fur, a soft blend of white and apricot, is highlighted by the golden sunlight, its floppy ears gently swaying in the breeze. Its happy expression and wagging tail convey pure joy and summer bliss. The vibrant pink flamingo adds a whimsical touch, creating a picture-perfect image of carefree fun in the LA sunshine. - + Prompt: A cinematic shot captures a fluffy Cockapoo, perched atop a vibrant pink flamingo float, in a sun-drenched Los Angeles swimming pool. The crystal-clear water sparkles under the bright California sun, reflecting the playful scene. The Cockapoo's fur, a soft blend of white and apricot, is highlighted by the golden sunlight, its floppy ears gently swaying in the breeze. Its happy expression and wagging tail convey pure joy and summer bliss. The vibrant pink flamingo adds a whimsical touch, creating a picture-perfect image of carefree fun in the LA sunshine. Prompt: A cinematic shot captures a fluffy Cockapoo, perched atop a vibrant pink flamingo float, in a sun-drenched Los Angeles swimming pool. The crystal-clear water sparkles under the bright California sun, reflecting the playful scene. The Cockapoo's fur, a soft blend of white and apricot, is highlighted by the golden sunlight, its floppy ears gently swaying in the breeze. Its happy expression and wagging tail convey pure joy and summer bliss. The vibrant pink flamingo adds a whimsical touch, creating a picture-perfect image of carefree fun in the LA sunshine. View on YouTube Pause video Play video Prompt: A cinematic, high-action tracking shot follows an incredibly cute dachshund wearing swimming goggles as it leaps into a crystal-clear pool. The camera plunges underwater with the dog, capturing the joyful moment of submersion and the ensuing flurry of paddling with adorable little paws. Sunlight filters through the water, illuminating the dachshund's sleek, wet fur and highlighting the determined expression on its face. The shot is filled with the vibrant blues and greens of the pool water, creating a dynamic and visually stunning sequence that captures the pure joy and energy of the swimming dachshund. - + Prompt: A cinematic, high-action tracking shot follows an incredibly cute dachshund wearing swimming goggles as it leaps into a crystal-clear pool. The camera plunges underwater with the dog, capturing the joyful moment of submersion and the ensuing flurry of paddling with adorable little paws. Sunlight filters through the water, illuminating the dachshund's sleek, wet fur and highlighting the determined expression on its face. The shot is filled with the vibrant blues and greens of the pool water, creating a dynamic and visually stunning sequence that captures the pure joy and energy of the swimming dachshund. Prompt: A cinematic, high-action tracking shot follows an incredibly cute dachshund wearing swimming goggles as it leaps into a crystal-clear pool. The camera plunges underwater with the dog, capturing the joyful moment of submersion and the ensuing flurry of paddling with adorable little paws. Sunlight filters through the water, illuminating the dachshund's sleek, wet fur and highlighting the determined expression on its face. The shot is filled with the vibrant blues and greens of the pool water, creating a dynamic and visually stunning sequence that captures the pure joy and energy of the swimming dachshund. View on YouTube Veo represents a significant step forward in high-quality video generation. Veo represents a significant step forward in high-quality video generation. Limitations While Veo 2 demonstrates incredible progress, creating realistic, dynamic, or intricate videos, and maintaining complete consistency throughout complex scenes or those with complex motion, remains a challenge. We’ll continue to develop and refine performance in these areas. Pause video Play video Prompt: The camera moves in a slow dolly shot, revealing the opulence of a Renaissance palace chamber adorned with gold-inlaid furniture, velvet drapes, and chandeliers casting soft, flickering light. A queen sits motionless at a gilded desk, her crimson silk gown cascading onto the floor like spilled blood. On the desk lies an unsigned letter, its edges curled with age. The camera frames her from behind, catching the reflection of her stoic face in a massive, ornate mirror. In the background, courtiers murmur, their silhouettes dancing like ghosts in the candlelight. The room feels heavy, every gilded detail amplifying an air of betrayal and paranoia. The color palette alternates between deep, regal reds and cold golds, with chiaroscuro lighting intensifying the drama. Shot on 70mm film for rich texture, evoking the grandeur of historical masterpieces. - + Prompt: The camera moves in a slow dolly shot, revealing the opulence of a Renaissance palace chamber adorned with gold-inlaid furniture, velvet drapes, and chandeliers casting soft, flickering light. A queen sits motionless at a gilded desk, her crimson silk gown cascading onto the floor like spilled blood. On the desk lies an unsigned letter, its edges curled with age. The camera frames her from behind, catching the reflection of her stoic face in a massive, ornate mirror. In the background, courtiers murmur, their silhouettes dancing like ghosts in the candlelight. The room feels heavy, every gilded detail amplifying an air of betrayal and paranoia. The color palette alternates between deep, regal reds and cold golds, with chiaroscuro lighting intensifying the drama. Shot on 70mm film for rich texture, evoking the grandeur of historical masterpieces. Prompt: The camera moves in a slow dolly shot, revealing the opulence of a Renaissance palace chamber adorned with gold-inlaid furniture, velvet drapes, and chandeliers casting soft, flickering light. A queen sits motionless at a gilded desk, her crimson silk gown cascading onto the floor like spilled blood. On the desk lies an unsigned letter, its edges curled with age. The camera frames her from behind, catching the reflection of her stoic face in a massive, ornate mirror. In the background, courtiers murmur, their silhouettes dancing like ghosts in the candlelight. The room feels heavy, every gilded detail amplifying an air of betrayal and paranoia. The color palette alternates between deep, regal reds and cold golds, with chiaroscuro lighting intensifying the drama. Shot on 70mm film for rich texture, evoking the grandeur of historical masterpieces. Pause video Play video Prompt: A tracking shot, with the subject centered in the frame, follows an ice skater gliding across an ice rink that appears to be floating amidst the clouds. The skater, clad in a flowing white costume that ripples with every move, exudes an ethereal grace. The camera smoothly keeps pace, capturing their every movement with a dreamlike quality. The background is a swirling canvas of pastel colors and soft, shifting clouds, creating a sense of otherworldly wonder. The skater's serene expression and the whisper-quiet sound of their blades on the ice add to the magical atmosphere. The overall impression is one of ethereal beauty and effortless movement, set against a backdrop of pure fantasy. - + Prompt: A tracking shot, with the subject centered in the frame, follows an ice skater gliding across an ice rink that appears to be floating amidst the clouds. The skater, clad in a flowing white costume that ripples with every move, exudes an ethereal grace. The camera smoothly keeps pace, capturing their every movement with a dreamlike quality. The background is a swirling canvas of pastel colors and soft, shifting clouds, creating a sense of otherworldly wonder. The skater's serene expression and the whisper-quiet sound of their blades on the ice add to the magical atmosphere. The overall impression is one of ethereal beauty and effortless movement, set against a backdrop of pure fantasy. Prompt: A tracking shot, with the subject centered in the frame, follows an ice skater gliding across an ice rink that appears to be floating amidst the clouds. The skater, clad in a flowing white costume that ripples with every move, exudes an ethereal grace. The camera smoothly keeps pace, capturing their every movement with a dreamlike quality. The background is a swirling canvas of pastel colors and soft, shifting clouds, creating a sense of otherworldly wonder. The skater's serene expression and the whisper-quiet sound of their blades on the ice add to the magical atmosphere. The overall impression is one of ethereal beauty and effortless movement, set against a backdrop of pure fantasy. Pause video Play video Prompt: A wide, static shot re-establishes the entire skatepark, showcasing the various features and obstacles. The setting sun casts long, dramatic shadows across the park, creating a heightened sense of anticipation for the skateboarder's final, and most challenging, trick. - + Prompt: A wide, static shot re-establishes the entire skatepark, showcasing the various features and obstacles. The setting sun casts long, dramatic shadows across the park, creating a heightened sense of anticipation for the skateboarder's final, and most challenging, trick. Prompt: A wide, static shot re-establishes the entire skatepark, showcasing the various features and obstacles. The setting sun casts long, dramatic shadows across the park, creating a heightened sense of anticipation for the skateboarder's final, and most challenging, trick. Veo 2 was made possible by key research and engineering contributions from Agrim Gupta, Ali Razavi, Ankush Gupta, Dumitru Erhan, Eric Lau, Frank Belletti, Gabe Barth-Maron, Hakan Erdogan, Hakim Sidahmed, Henna Nandwani, Hernan Moraldo, Hyunjik Kim, Jeff Donahue, José Lezama, Kory Mathewson, Kurtis David, Marc van Zee, Medhini Narasimhan, Miaosen Wang, Mohammad Babaeizadeh, Nelly Papalampidi, Nick Pezzotti, Nilpa Jha, Parker Barnes, Pieter-Jan Kindermans, Rachel Hornung, Ruben Villegas, Ryan Poplin, Salah Zaiem, Sander Dieleman, Sayna Ebrahimi, Scott Wisdom, Serena Zhang, Shlomi Fruchter, Weizhe Hua, Xinchen Yan, Yuqing Du and Yutian Chen. All the clips were generated directly with Veo without modifications by Eleni Shaw, Signe Nørly, Andeep Toor, Gregory Shaw, Matthieu Kim Lorrain, and Irina Blok. We extend our gratitude to Abhishek Sharma, Adams Yu, Ahmed Chowdhury, Aida Nematzadeh, Aleksander Hołyński, Andrew Audibert, Andrew Marmon, Andrew Pierson, Ariel Ephrat, Ashley Feden, Austin Tarango, Austin Waters, Ben Poole, Bryan Seybold, Daniel Tanis, David Bridson, David Reid, David Yao, Dirk Robinson, Emanuele Bugliarello, Evgeny Gladchenko, Frank Perbet, Frankie Garcia, Hadi Hashemi, Hongliang Fei, Huisheng Wang, Inbar Mosseri, Ira Ktena, Jakob Bauer, Jenny Brennan, Joana Iljazi, John Zhang, Jonas Adler, Jordi Pont-Tuset, Josh Newlan, Junyoung Chung, Kan Chen, Karol Langner, Katie Zhang, Keyang Xu, Lasse Espeholt, Lluis Castrejon, Luis C. Cobo, Mahyar Bordbar, Michael Mathieu, Mingda Zhang, Mitchell McIntire, Mohammad Taghi Saffar, Mukul Bhutani, Nikhil Khadke, Nikos Kolotouros, Norman Casagrande, Oliver Wang, Oliver Woodman, Omer Tov, Orly Liba, Pankil Botadra, Petko Georgiev, Piyush Kumar, RJ Mical, Roni Paiss, Seliem El-Sayed, Shiran Zada, Shixin Luo, Simon Wang, Soňa Mokrá, Srinivas Tadepalli, Tatiana López, Thomas Kipf, Tobias Pfaff, Tom Eccles, Tom Hume, Vikas Verma, Will Hawkins, Woohyun Han, Xinyu Wang, Xuhui Jia, Yelin Kim, Yilin Gao, Yori Zwols, Yuchi Liu, Yukun Zhu, Yulia Rubanova, Yusuf Aytar, Zarana Parekh, Zhenkai Zhu and Zu Kim for their invaluable partnership in developing and refining key components of this project. Special thanks to Douglas Eck, Aäron van den Oord, Eli Collins, Koray Kavukcuoglu and Demis Hassabis for their insightful guidance and support throughout the research process. We also acknowledge the many other individuals who contributed across Google DeepMind and our partners at Google. View citation Get the latest updates Sign up for news on the latest innovations from Google DeepMind. Email address Please enter a valid email (e.g., \"name@example.com\") I accept Google's Terms and Conditions and acknowledge that my information will be used in accordance with Google's Privacy Policy. Sign up More technologies Imagen Our highest quality text-to-image model Gemini Our new AI model for the agentic era SynthID Robust and scalable tool for watermarking and identifying AI-generated images Explore our other teams and product areas Google AI Google AI for Developers Google AI Studio Gemini Google Cloud Google Labs Footer links Follow us About About Google DeepMind Responsibility & Safety Research Technologies Blog Careers Learn more Gemini Veo Imagen SynthID Sign up for updates on our latest innovations Email address Please enter a valid email (e.g., \"name@example.com\") I accept Google's Terms and Conditions and acknowledge that my information will be used in accordance with Google's Privacy Policy. Sign up About Google Google products Privacy Terms Cookies management controls",
      "title": "https://opentools.ai/news/freepik-and-google-unleash-veo-2-the-future-of-ai-video-generation-is-here"
    },
    {
      "url": "https://huggingface.co/blog/video_gen",
      "content": "A Dive into Text-to-Video Models Hugging Face Models Datasets Spaces Posts Docs Enterprise Pricing Log In Sign Up Back to Articles Text-to-Video: The Task, Challenges and the Current State Published May 8, 2023 Update on GitHub Upvote 30 +24 adirik Alara Dirik Video samples generated with ModelScope. Text-to-Video vs. Text-to-Image How to Generate Videos from Text? Datasets Text-to-Video at Hugging Face Hugging Face Demos Community Contributions and Open Source Text-to-Video Projects Conclusion Text-to-video is next in line in the long list of incredible advances in generative models. As self-descriptive as it is, text-to-video is a fairly new computer vision task that involves generating a sequence of images from text descriptions that are both temporally and spatially consistent. While this task might seem extremely similar to text-to-image, it is notoriously more difficult. How do these models work, how do they differ from text-to-image models, and what kind of performance can we expect from them? In this blog post, we will discuss the past, present, and future of text-to-video models. We will start by reviewing the differences between the text-to-video and text-to-image tasks, and discuss the unique challenges of unconditional and text-conditioned video generation. Additionally, we will cover the most recent developments in text-to-video models, exploring how these methods work and what they are capable of. Finally, we will talk about what we are working on at Hugging Face to facilitate the integration and use of these models and share some cool demos and resources both on and outside of the Hugging Face Hub. Examples of videos generated from various text description inputs, image taken from Make-a-Video. Text-to-Video vs. Text-to-Image With so many recent developments, it can be difficult to keep up with the current state of text-to-image generative models. Let's do a quick recap first. Just two years ago, the first open-vocabulary, high-quality text-to-image generative models emerged. This first wave of text-to-image models, including VQGAN-CLIP, XMC-GAN, and GauGAN2, all had GAN architectures. These were quickly followed by OpenAI's massively popular transformer-based DALL-E in early 2021, DALL-E 2 in April 2022, and a new wave of diffusion models pioneered by Stable Diffusion and Imagen. The huge success of Stable Diffusion led to many productionized diffusion models, such as DreamStudio and RunwayML GEN-1, and integration with existing products, such as Midjourney. Despite the impressive capabilities of diffusion models in text-to-image generation, diffusion and non-diffusion based text-to-video models are significantly more limited in their generative capabilities. Text-to-video are typically trained on very short clips, meaning they require a computationally expensive and slow sliding window approach to generate long videos. As a result, these models are notoriously difficult to deploy and scale and remain limited in context and length. The text-to-video task faces unique challenges on multiple fronts. Some of these main challenges include: Computational challenges: Ensuring spatial and temporal consistency across frames creates long-term dependencies that come with a high computation cost, making training such models unaffordable for most researchers. Lack of high-quality datasets: Multi-modal datasets for text-to-video generation are scarce and often sparsely annotated, making it difficult to learn complex movement semantics. Vagueness around video captioning: Describing videos in a way that makes them easier for models to learn from is an open question. More than a single short text prompt is required to provide a complete video description. A generated video must be conditioned on a sequence of prompts or a story that narrates what happens over time. In the next section, we will discuss the timeline of developments in the text-to-video domain and the various methods proposed to address these challenges separately. On a higher level, text-to-video works propose one of these: New, higher-quality datasets that are easier to learn from. Methods to train such models without paired text-video data. More computationally efficient methods to generate longer and higher resolution videos. How to Generate Videos from Text? Let's take a look at how text-to-video generation works and the latest developments in this field. We will explore how text-to-video models have evolved, following a similar path to text-to-image research, and how the specific challenges of text-to-video generation have been tackled so far. Like the text-to-image task, early work on text-to-video generation dates back only a few years. Early research predominantly used GAN and VAE-based approaches to auto-regressively generate frames given a caption (see Text2Filter and TGANs-C). While these works provided the foundation for a new computer vision task, they are limited to low resolutions, short-range, and singular, isolated motions. Initial text-to-video models were extremely limited in resolution, context and length, image taken from TGANs-C. Taking inspiration from the success of large-scale pretrained transformer models in text (GPT-3) and image (DALL-E), the next surge of text-to-video generation research adopted transformer architectures. Phenaki, Make-A-Video, NUWA, VideoGPT and CogVideo all propose transformer-based frameworks, while works such as TATS propose hybrid methods that combine VQGAN for image generation and a time-sensitive transformer module for sequential generation of frames. Out of this second wave of works, Phenaki is particularly interesting as it enables generating arbitrary long videos conditioned on a sequence of prompts, in other words, a story line. Similarly, NUWA-Infinity proposes an autoregressive over autoregressive generation mechanism for infinite image and video synthesis from text inputs, enabling the generation of long, HD quality videos. However, neither Phenaki or NUWA models are publicly available. Phenaki features a transformer-based architecture, image taken from here. The third and current wave of text-to-video models features predominantly diffusion-based architectures. The remarkable success of diffusion models in diverse, hyper-realistic, and contextually rich image generation has led to an interest in generalizing diffusion models to other domains such as audio, 3D, and, more recently, video. This wave of models is pioneered by Video Diffusion Models (VDM), which extend diffusion models to the video domain, and MagicVideo, which proposes a framework to generate video clips in a low-dimensional latent space and reports huge efficiency gains over VDM. Another notable mention is Tune-a-Video, which fine-tunes a pretrained text-to-image model with a single text-video pair and enables changing the video content while preserving the motion. The continuously expanding list of text-to-video diffusion models that followed include Video LDM, Text2Video-Zero, Runway Gen1 and Gen2, and NUWA-XL. Text2Video-Zero is a text-guided video generation and manipulation framework that works in a fashion similar to ControlNet. It can directly generate (or edit) videos based on text inputs, as well as combined text-pose or text-edge data inputs. As implied by its name, Text2Video-Zero is a zero-shot model that combines a trainable motion dynamics module with a pre-trained text-to-image Stable Diffusion model without using any paired text-video data. Similarly to Text2Video-Zero, Runway’s Gen-1 and Gen-2 models enable synthesizing videos guided by content described through text or images. Most of these works are trained on short video clips and rely on autoregressive generation with a sliding window to generate longer videos, inevitably resulting in a context gap. NUWA-XL addresses this issue and proposes a “diffusion over diffusion” method to train models on 3376 frames. Finally, there are open-source text-to-video models and frameworks such as Alibaba / DAMO Vision Intelligence Lab’s ModelScope and Tencel’s VideoCrafter, which haven't been published in peer-reviewed conferences or journals. Datasets Like other vision-language models, text-to-video models are typically trained on large paired datasets videos and text descriptions. The videos in these datasets are typically split into short, fixed-length chunks and often limited to isolated actions with a few objects. While this is partly due to computational limitations and partly due to the difficulty of describing video content in a meaningful way, we see that developments in multimodal video-text datasets and text-to-video models are often entwined. While some work focuses on developing better, more generalizable datasets that are easier to learn from, works such as Phenaki explore alternative solutions such as combining text-image pairs with text-video pairs for the text-to-video task. Make-a-Video takes this even further by proposing using only text-image pairs to learn what the world looks like and unimodal video data to learn spatio-temporal dependencies in an unsupervised fashion. These large datasets experience similar issues to those found in text-to-image datasets. The most commonly used text-video dataset, WebVid, consists of 10.7 million pairs of text-video pairs (52K video hours) and contains a fair amount of noisy samples with irrelevant video descriptions. Other datasets try to overcome this issue by focusing on specific tasks or domains. For example, the Howto100M dataset consists of 136M video clips with captions that describe how to perform complex tasks such as cooking, handcrafting, gardening, and fitness step-by-step. Similarly, the QuerYD dataset focuses on the event localization task such that the captions of videos describe the relative location of objects and actions in detail. CelebV-Text is a large-scale facial text-video dataset of over 70K videos to generate videos with realistic faces, emotions, and gestures. Text-to-Video at Hugging Face Using Hugging Face Diffusers, you can easily download, run and fine-tune various pretrained text-to-video models, including Text2Video-Zero and ModelScope by Alibaba / DAMO Vision Intelligence Lab. We are currently working on integrating other exciting works into Diffusers and 🤗 Transformers. Hugging Face Demos At Hugging Face, our goal is to make it easier to use and build upon state-of-the-art research. Head over to our hub to see and play around with Spaces demos contributed by the 🤗 team, countless community contributors and research authors. At the moment, we host demos for VideoGPT, CogVideo, ModelScope Text-to-Video, and Text2Video-Zero with many more to come. To see what we can do with these models, let's take a look at the Text2Video-Zero demo. This demo not only illustrates text-to-video generation but also enables multiple other generation modes for text-guided video editing and joint conditional video generation using pose, depth and edge inputs along with text prompts. Apart from using demos to experiment with pretrained text-to-video models, you can also use the Tune-a-Video training demo to fine-tune an existing text-to-image model with your own text-video pair. To try it out, upload a video and enter a text prompt that describes the video. Once the training is done, you can upload it to the Hub under the Tune-a-Video community or your own username, publicly or privately. Once the training is done, simply head over to the Run tab of the demo to generate videos from any text prompt. All Spaces on the 🤗 Hub are Git repos you can clone and run on your local or deployment environment. Let’s clone the ModelScope demo, install the requirements, and run it locally. git clone https://huggingface.co/spaces/damo-vilab/modelscope-text-to-video-synthesis cd modelscope-text-to-video-synthesis pip install -r requirements.txt python app.py And that's it! The Modelscope demo is now running locally on your computer. Note that the ModelScope text-to-video model is supported in Diffusers and you can directly load and use the model to generate new videos with a few lines of code. ``` import torch from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler from diffusers.utils import export_to_video pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\") pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config) pipe.enable_model_cpu_offload() prompt = \"Spiderman is surfing\" video_frames = pipe(prompt, num_inference_steps=25).frames video_path = export_to_video(video_frames) ``` Community Contributions and Open Source Text-to-Video Projects Finally, there are various open source projects and models that are not on the hub. Some notable mentions are Phil Wang’s (aka lucidrains) unofficial implementations of Imagen, Phenaki, NUWA, Make-a-Video and Video Diffusion Models. Another exciting project by ExponentialML builds on top of 🤗 diffusers to finetune ModelScope Text-to-Video. Conclusion Text-to-video research is progressing exponentially, but existing work is still limited in context and faces many challenges. In this blog post, we covered the constraints, unique challenges and the current state of text-to-video generation models. We also saw how architectural paradigms originally designed for other tasks enable giant leaps in the text-to-video generation task and what this means for future research. While the developments are impressive, text-to-video models still have a long way to go compared to text-to-image models. Finally, we also showed how you can use these models to perform various tasks using the demos available on the Hub or as a part of 🤗 Diffusers pipelines. That was it! We are continuing to integrate the most impactful computer vision and multi-modal models and would love to hear back from you. To stay up to date with the latest news in computer vision and multi-modal research, you can follow us on Twitter: @adirik, @a_e_roberts, @osanseviero, @risingsayak and @huggingface. More Articles from our Blog Practical 3D Asset Generation: A Step-by-Step Guide --------------------------------------------------- By dylanebert August 1, 2023 • 6 Stable Diffusion in JAX/Flax 🚀 ------------------------------- By pcuenca October 13, 2022 • 2 Community buthjjn22 days ago A man in an aeroplane + Reply Dammy11217 days ago This comment has been hidden + EditPreview Upload images, audio, and videos by dragging in the text input, pasting, or clicking here. Tap or paste here to upload images Comment· Sign up or log in to comment Upvote 30 +18 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs",
      "title": "https://huggingface.co/blog/video_gen"
    },
    {
      "url": "https://www.youtube.com/watch?v=-2tRBIzn07Y",
      "content": "YouTube • NaN / NaN Back Skip navigation Search Search Search with your voice Sign in Home HomeShorts Shorts Subscriptions SubscriptionsYou YouHistory History Try searching to get started Start watching videos to help us build a feed of videos you'll love. Search Info Shopping Tap to unmute 2x If playback doesn't begin shortly, try restarting your device. • You're signed out Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer. CancelConfirm Share Include playlist An error occurred while retrieving sharing information. Please try again later. Watch later Share Copy link 0:00 / •Watch full videoLive • • Scroll for details … NaN / NaN",
      "title": "https://www.youtube.com/watch?v=-2tRBIzn07Y"
    },
    {
      "url": "https://9meters.com/technology/ai/what-is-google-veo-2",
      "content": "Updates to Veo, Imagen and VideoFX, plus introducing Whisk in Google Labs [{\"model\": \"blogsurvey.survey\", \"pk\": 5, \"fields\": {\"name\": \"Sentiment Change - All Articles - Nov 2024\", \"survey_id\": \"sentiment-change-all-articles-nov-2024_241031\", \"scroll_depth_trigger\": 75, \"previous_survey\": null, \"display_rate\": 50, \"thank_message\": \"Thank you!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"c32a4772-5575-4985-814a-afd8d15d5d6d\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {\\\"question\\\": \\\"After reading this article, how has your perception of Google changed?\\\", \\\"responses\\\": [{\\\"id\\\": \\\"cb111cda-60ba-4ac5-8260-17c5326e485b\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Gotten better\\\"}, {\\\"id\\\": \\\"d8864abb-689a-4b52-b021-449af0b0a7c6\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Gotten worse\\\"}, {\\\"id\\\": \\\"f169d98d-1731-4efc-be90-3cd379a2a63e\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Stayed the same\\\"}, {\\\"id\\\": \\\"701d8c63-affe-4f44-85f0-71538a310d65\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Don't know\\\"}]}}]\", \"target_article_pages\": true}}] Skip to main content The Keyword State-of-the-art video and image generation with Veo 2 and Imagen 3 Share TwitterFacebookLinkedInMail Copy link Home Product news Product news Android, Chrome & Play Android Chrome Chromebooks Google Play Wear OS Platforms & Devices Fitbit Google Nest Pixel Explore & Get Answers Gemini Maps News Search Shopping Connect & Communicate Classroom Photos Registry Translate In The Cloud Google Workspace More on the Cloud Blog Google Cloud See all product updates Android, Chrome & Play Android Chrome Chromebooks Google Play Wear OS Platforms & Devices Fitbit Google Nest Pixel Explore & Get Answers Gemini Maps News Search Shopping Connect & Communicate Classroom Photos Registry Translate In The Cloud Google Workspace More on the Cloud Blog Google Cloud See all product updates Company news Company news Outreach & initiatives Arts & Culture Education Entrepreneurs Public Policy Sustainability See all Technology AI Developers Health Google DeepMind Google Labs Safety and security See all Inside Google Data centers and infrastructure Doodles Googlers Life at Google See all Around the globe Google in Asia Google in Europe Google in Latin America See all Authors Sundar Pichai, CEO Ruth Porat, President & Chief Investment Officer Kent Walker, SVP James Manyika, SVP See all Outreach & initiatives Arts & Culture Education Entrepreneurs Public Policy Sustainability See all * Technology AI Developers Health Google DeepMind Google Labs Safety and security See all * Inside Google Data centers and infrastructure Doodles Googlers Life at Google See all * Around the globe Google in Asia Google in Europe Google in Latin America See all * Authors Sundar Pichai, CEO Ruth Porat, President & Chief Investment Officer Kent Walker, SVP James Manyika, SVP See all Feed Subscribe Subscribe The Keyword Home Product news Product news Android, Chrome & Play Android Chrome Chromebooks Google Play Wear OS Platforms & Devices Fitbit Google Nest Pixel Explore & Get Answers Gemini Maps News Search Shopping Connect & Communicate Classroom Photos Registry Translate In The Cloud Google Workspace More on the Cloud Blog Google Cloud See all product updates Company news Company news Outreach & initiatives Arts & Culture Education Entrepreneurs Public Policy Sustainability See all Technology AI Developers Health Google DeepMind Google Labs Safety and security See all Inside Google Data centers and infrastructure Doodles Googlers Life at Google See all Around the globe Google in Asia Google in Europe Google in Latin America See all Authors Sundar Pichai, CEO Ruth Porat, President & Chief Investment Officer Kent Walker, SVP James Manyika, SVP See all Feed Press corner RSS feed Subscribe Breadcrumb Technology Google Labs State-of-the-art video and image generation with Veo 2 and Imagen 3 Dec 16, 2024 ·[[read-time]] min read Share TwitterFacebookLinkedInMail Copy link We’re announcing new versions of Veo and Imagen, and introducing our latest experiment in image generation: Whisk. Aäron van den Oord Research Scientist, Google DeepMind Elias Roman Senior Director, Product Management, Google Labs Share TwitterFacebookLinkedInMail Copy link Earlier this year, we introduced our video generation model, Veo, and our latest image generation model, Imagen 3. Since then, it’s been exciting to watch people bring their ideas to life with help from these models: YouTube creators are exploring the creative possibilities of video backgrounds for their YouTube Shorts, enterprise customers are enhancing creative workflows on Vertex AI and creatives are using VideoFX and ImageFX to tell their stories. Together with collaborators ranging from filmmakers to businesses, we’re continuing to develop and evolve these technologies. Today we're introducing a new video model, Veo 2, and the latest version of Imagen 3, both of which achieve state-of-the-art results. These models are now available in VideoFX, ImageFX and our newest Labs experiment, Whisk. Veo 2: state-of-the-art video generation Veo 2 creates incredibly high-quality videos in a wide range of subjects and styles. In head-to-head comparisons judged by human raters, Veo 2 achieved state-of-the-art results against leading models. It brings an improved understanding of real-world physics and the nuances of human movement and expression, which helps improve its detail and realism overall. Veo 2 understands the unique language of cinematography: ask it for a genre, specify a lens, suggest cinematic effects and Veo 2 will deliver — at resolutions up to 4K, and extended to minutes in length. Ask for a low-angle tracking shot that glides through the middle of a scene, or a close-up shot on the face of a scientist looking through her microscope, and Veo 2 creates it. Suggest “18mm lens” in your prompt and Veo 2 knows to craft the wide angle shot that this lens is known for, or blur out the background and focus on your subject by putting \"shallow depth of field\" in your prompt. Video format not supported Cinematic shot of a female doctor in a dark yellow hazmat suit, illuminated by the harsh fluorescent light of a laboratory. The camera slowly zooms in on her face, panning gently to emphasize the worry and anxiety etched across her brow. She is hunched over a lab table, peering intently into a microscope, her gloved hands carefully adjusting the focus. The muted color palette of the scene, dominated by the sickly yellow of the suit and the sterile steel of the lab, underscores the gravity of the situation and the weight of the unknown she is facing. The shallow depth of field focuses on the fear in her eyes, reflecting the immense pressure and responsibility she bears. Examples of Veo 2's high-quality video generation capabilities. All videos were generated by Veo 2 and have not been modified. Video format not supported This medium shot, with a shallow depth of field, portrays an adorable cartoon girl with wavy brown hair and lots of character, sitting upright in a 1980s kitchen. Her hair is medium length and wavy. She has a small, slightly upturned nose, and small, rounded ears. She is very animated and excited as she talks to the camera and lighting and giggling with a huge grin. Examples of Veo 2's high-quality video generation capabilities. All videos were generated by Veo 2 and have not been modified. Video format not supported The camera floats gently through rows of pastel-painted wooden beehives, buzzing honeybees gliding in and out of frame. The motion settles on the refined farmer standing at the center, his pristine white beekeeping suit gleaming in the golden afternoon light. He lifts a jar of honey, tilting it slightly to catch the light. Behind him, tall sunflowers sway rhythmically in the breeze, their petals glowing in the warm sunlight. The camera tilts upward to reveal a retro farmhouse with mint-green shutters, its walls dappled with shadows from swaying trees. Shot with a 35mm lens on Kodak Portra 400 film, the golden light creates rich textures on the farmer’s gloves, marmalade jar, and weathered wood of the beehives. Examples of Veo 2's high-quality video generation capabilities. All videos were generated by Veo 2 and have not been modified. Video format not supported A low-angle shot captures a flock of pink flamingos gracefully wading in a lush, tranquil lagoon. The vibrant pink of their plumage contrasts beautifully with the verdant green of the surrounding vegetation and the crystal-clear turquoise water. Sunlight glints off the water's surface, creating shimmering reflections that dance on the flamingos' feathers. The birds' elegant, curved necks are submerged as they walk through the shallow water, their movements creating gentle ripples that spread across the lagoon. The composition emphasizes the serenity and natural beauty of the scene, highlighting the delicate balance of the ecosystem and the inherent grace of these magnificent birds. The soft, diffused light of early morning bathes the entire scene in a warm, ethereal glow. Examples of Veo 2's high-quality video generation capabilities. All videos were generated by Veo 2 and have not been modified. Video format not supported A perfect cube rotates in the center of a soft, foggy void. The surface shifts between different hyper-real textures—smooth marble, velvety suede, hammered brass, and raw concrete. Each material reveals subtle details: marble veins slowly spreading, suede fibers brushing with wind, brass tarnishing in slow motion, and concrete crumbling to reveal polished stone inside. Ends with a soft glow surrounding the cube as it transitions to a smooth mirrored surface, reflecting infinity. Examples of Veo 2's high-quality video generation capabilities. All videos were generated by Veo 2 and have not been modified. Video format not supported A cinematic shot captures a fluffy Cockapoo, perched atop a vibrant pink flamingo float, in a sun-drenched Los Angeles swimming pool. The crystal-clear water sparkles under the bright California sun, reflecting the playful scene. The Cockapoo's fur, a soft blend of white and apricot, is highlighted by the golden sunlight, its floppy ears gently swaying in the breeze. Its happy expression and wagging tail convey pure joy and summer bliss. The vibrant pink flamingo adds a whimsical touch, creating a picture-perfect image of carefree fun in the LA sunshine. Examples of Veo 2's high-quality video generation capabilities. All videos were generated by Veo 2 and have not been modified. Video format not supported The sun rises slowly behind a perfectly plated breakfast scene. Thick, golden maple syrup pours in slow motion over a stack of fluffy pancakes, each one releasing a soft, warm steam cloud. A close-up of crispy bacon sizzles, sending tiny embers of golden grease into the air. Coffee pours in smooth, swirling motion into a crystal-clear cup, filling it with deep brown layers of crema. Scene ends with a camera swoop into a fresh-cut orange, revealing its bright, juicy segments in stunning macro detail. Examples of Veo 2's high-quality video generation capabilities. All videos were generated by Veo 2 and have not been modified. 1 2 3 4 5 6 7 While video models often “hallucinate” unwanted details — extra fingers or unexpected objects, for example — Veo 2 produces these less frequently, making outputs more realistic. Our commitment to safety and responsible development has guided Veo 2. We have been intentionally measured in growing Veo’s availability, so we can help identify, understand and improve the model’s quality and safety while slowly rolling it out via VideoFX, YouTube and Vertex AI. Just like the rest of our image and video generation models, Veo 2 outputs include an invisible SynthID watermark that helps identify them as AI-generated, helping reduce the chances of misinformation and misattribution. Today, we're bringing our new Veo 2 capabilities to our Google Labs video generation tool, VideoFX, and expanding the number of users who can access it. Visit Google Labs to sign up for the waitlist. We also plan to expand Veo 2 to YouTube Shorts and other products next year. Note: Find prompts for all videos at the bottom of this post: Scientist1, Cartoon character2, Bees3, Flamingos4, Cube5, Dog6, Pancakes7 Imagen 3: state-of-the-art image generation We've also improved our Imagen 3 image-generation model, which now generates brighter, better composed images. It can now render more diverse art styles with greater accuracy — from photorealism to impressionism, from abstract to anime. This upgrade also follows prompts more faithfully, and renders richer details and textures. In side-by-side comparisons of outputs by human raters against leading image generation models, Imagen 3 achieved state-of-the-art results. Starting today, the latest Imagen 3 model will globally roll out in ImageFX, our image generation tool from Google Labs, to more than 100 countries. Visit ImageFX to get started. Examples of Imagen 3's rich detail and image quality composition Examples of Imagen 3's rich detail and image quality composition Examples of Imagen 3's rich detail and image quality composition Examples of Imagen 3's rich detail and image quality composition Examples of Imagen 3's rich detail and image quality composition 1 2 3 4 5 Note: Find prompts for all images at the bottom of this post: Potter8, Squirrel9, Train station10, Woman11, Strawberry bird12 Whisk: a fun new tool that lets you prompt with images to visualize your ideas Whisk, our newest experiment from Google Labs, lets you input or create images that convey the subject, scene and style you have in mind. Then, you can bring them together and remix them to create something uniquely your own, from a digital plushie to an enamel pin or sticker. Under the hood, Whisk combines our latest Imagen 3 model with Gemini’s visual understanding and description capabilities. The Gemini model automatically writes a detailed caption of your images, and it then feeds those descriptions into Imagen 3. This process allows you to easily remix your subjects, scenes and styles in fun, new ways. 10:25 Whisk lets you quickly visualize and remix ideas. Whisk is launching in the U.S. today. Read more about Whisk and try it out at labs.google/Whisk. POSTED IN: Google Labs Google DeepMind Read Article More Information 1 Veo 2 prompt: Cinematic shot of a female doctor in a dark yellow hazmat suit, illuminated by the harsh fluorescent light of a laboratory. The camera slowly zooms in on her face, panning gently to emphasize the worry and anxiety etched across her brow. She is hunched over a lab table, peering intently into a microscope, her gloved hands carefully adjusting the focus. The muted color palette of the scene, dominated by the sickly yellow of the suit and the sterile steel of the lab, underscores the gravity of the situation and the weight of the unknown she is facing. The shallow depth of field focuses on the fear in her eyes, reflecting the immense pressure and responsibility she bears. 2 Veo 2 prompt: This medium shot, with a shallow depth of field, portrays an adorable cartoon girl with wavy brown hair and lots of character, sitting upright in a 1980s kitchen. Her hair is medium length and wavy. She has a small, slightly upturned nose, and small, rounded ears. She is very animated and excited as she talks to the camera and lighting and giggling with a huge grin. 3 Veo 2 prompt: The camera floats gently through rows of pastel-painted wooden beehives, buzzing honeybees gliding in and out of frame. The motion settles on the refined farmer standing at the center, his pristine white beekeeping suit gleaming in the golden afternoon light. He lifts a jar of honey, tilting it slightly to catch the light. Behind him, tall sunflowers sway rhythmically in the breeze, their petals glowing in the warm sunlight. The camera tilts upward to reveal a retro farmhouse with mint-green shutters, its walls dappled with shadows from swaying trees. Shot with a 35mm lens on Kodak Portra 400 film, the golden light creates rich textures on the farmer’s gloves, marmalade jar, and weathered wood of the beehives. 4 Veo 2 prompt: A low-angle shot captures a flock of pink flamingos gracefully wading in a lush, tranquil lagoon. The vibrant pink of their plumage contrasts beautifully with the verdant green of the surrounding vegetation and the crystal-clear turquoise water. Sunlight glints off the water's surface, creating shimmering reflections that dance on the flamingos' feathers. The birds' elegant, curved necks are submerged as they walk through the shallow water, their movements creating gentle ripples that spread across the lagoon. The composition emphasizes the serenity and natural beauty of the scene, highlighting the delicate balance of the ecosystem and the inherent grace of these magnificent birds. The soft, diffused light of early morning bathes the entire scene in a warm, ethereal glow. 5 Veo 2 prompt: A perfect cube rotates in the center of a soft, foggy void. The surface shifts between different hyper-real textures—smooth marble, velvety suede, hammered brass, and raw concrete. Each material reveals subtle details: marble veins slowly spreading, suede fibers brushing with wind, brass tarnishing in slow motion, and concrete crumbling to reveal polished stone inside. Ends with a soft glow surrounding the cube as it transitions to a smooth mirrored surface, reflecting infinity. 6 Veo 2 prompt: A cinematic shot captures a fluffy Cockapoo, perched atop a vibrant pink flamingo float, in a sun-drenched Los Angeles swimming pool. The crystal-clear water sparkles under the bright California sun, reflecting the playful scene. The Cockapoo's fur, a soft blend of white and apricot, is highlighted by the golden sunlight, its floppy ears gently swaying in the breeze. Its happy expression and wagging tail convey pure joy and summer bliss. The vibrant pink flamingo adds a whimsical touch, creating a picture-perfect image of carefree fun in the LA sunshine. 7 Veo 2 prompt: The sun rises slowly behind a perfectly plated breakfast scene. Thick, golden maple syrup pours in slow motion over a stack of fluffy pancakes, each one releasing a soft, warm steam cloud. A close-up of crispy bacon sizzles, sending tiny embers of golden grease into the air. Coffee pours in smooth, swirling motion into a crystal-clear cup, filling it with deep brown layers of crema. Scene ends with a camera swoop into a fresh-cut orange, revealing its bright, juicy segments in stunning macro detail. 8 Imagen 3 prompt: An extreme close-up of a craftsperson's hands shaping a glowing piece of pottery on a wheel. Threads of golden, luminous energy connect the potter’s hands to the clay, swirling dynamically with their movements. The workspace is filled with rich textures—dusty shelves lined with tools, scattered clay fragments, and beams of natural light piercing through wooden shutters. The interplay of light and energy creates an ethereal, almost magical atmosphere 9 Imagen 3 prompt: A close-up shot captures a winter wonderland scene – soft snowflakes fall on a snow-covered forest floor. Behind a frosted pine branch, a red squirrel sits, its bright orange fur a splash of color against the white. It holds a small hazelnut. As it enjoys its meal, it seems oblivious to the falling snow. 10 Imagen 3 prompt: A foggy 1940s European train station at dawn, framed by intricate wrought-iron arches and misted glass windows. Steam rises from the tracks, blending with dense fog. Two lovers stand in an emotional embrace near the train, backlit by the warm, amber glow of dim lanterns. The departing train is partially visible, its red tail lights fading into the mist. The woman wears a faded red coat and clutches a small leather diary, while the man is dressed in a weathered soldier’s uniform. Dust motes float in the air, illuminated by the soft golden backlight. The atmosphere is melancholic and timeless, evoking the bittersweet farewell of wartime cinema. 11 Imagen 3 prompt: A portrait of an Asian woman with neon green lights in the background, shallow depth of field. 12 Imagen 3 prompt: A close-up, macro photography stock photo of a strawberry intricately sculpted into the shape of a hummingbird in mid-flight, its wings a blur as it sips nectar from a vibrant, tubular flower. The backdrop features a lush, colorful garden with a soft, bokeh effect, creating a dreamlike atmosphere. The image is exceptionally detailed and captured with a shallow depth of field, ensuring a razor-sharp focus on the strawberry-hummingbird and gentle fading of the background. The high resolution, professional photographers style, and soft lighting illuminate the scene in a very detailed manner, professional color grading amplifies the vibrant colors and creates an image with exceptional clarity. The depth of field makes the hummingbird and flower stand out starkly against the bokeh background. Collapse Related stories Google Labs #### Whisk: Visualize and remix ideas using images and AI By Thomas Iljic & Nicole Brichtova Dec 16, 2024 Google Labs #### NotebookLM gets a new look, audio interactivity and a premium version By Steven Johnson Dec 13, 2024 Google DeepMind #### Introducing Gemini 2.0: our new AI model for the agentic era By Sundar Pichai & Demis Hassabis & Koray Kavukcuoglu Dec 11, 2024 Google Labs #### Listen to your first-ever 2024 Spotify Wrapped AI podcast, built with NotebookLM By Janakitti Ratana-Rueangsri Dec 04, 2024 Google DeepMind #### AlphaQubit tackles one of quantum computing’s biggest challenges By Google DeepMind and Quantum AI teams Nov 20, 2024 AI #### How songwriter Justin Tranter helped evolve Music AI Sandbox By Arathi Sethumadhavan Nov 19, 2024 . Let’s stay in touch. Get the latest news from Google in your inbox. Subscribe No thanks Follow Us Privacy Terms About Google Google Products About the Keyword Help *",
      "title": "https://9meters.com/technology/ai/what-is-google-veo-2"
    },
    {
      "url": "https://www.marktechpost.com/2024/12/17/google-released-state-of-the-art-veo-2-for-video-generation-and-improved-imagen-3-for-image-creation-setting-new-standards-with-4k-video-and-several-minutes-long-video-generation/",
      "content": "Updates to Veo, Imagen and VideoFX, plus introducing Whisk in Google Labs [{\"model\": \"blogsurvey.survey\", \"pk\": 5, \"fields\": {\"name\": \"Sentiment Change - All Articles - Nov 2024\", \"survey_id\": \"sentiment-change-all-articles-nov-2024_241031\", \"scroll_depth_trigger\": 75, \"previous_survey\": null, \"display_rate\": 50, \"thank_message\": \"Thank you!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"c32a4772-5575-4985-814a-afd8d15d5d6d\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {\\\"question\\\": \\\"After reading this article, how has your perception of Google changed?\\\", \\\"responses\\\": [{\\\"id\\\": \\\"cb111cda-60ba-4ac5-8260-17c5326e485b\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Gotten better\\\"}, {\\\"id\\\": \\\"d8864abb-689a-4b52-b021-449af0b0a7c6\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Gotten worse\\\"}, {\\\"id\\\": \\\"f169d98d-1731-4efc-be90-3cd379a2a63e\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Stayed the same\\\"}, {\\\"id\\\": \\\"701d8c63-affe-4f44-85f0-71538a310d65\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Don't know\\\"}]}}]\", \"target_article_pages\": true}}] Skip to main content The Keyword State-of-the-art video and image generation with Veo 2 and Imagen 3 Share TwitterFacebookLinkedInMail Copy link Home Product news Product news Android, Chrome & Play Android Chrome Chromebooks Google Play Wear OS Platforms & Devices Fitbit Google Nest Pixel Explore & Get Answers Gemini Maps News Search Shopping Connect & Communicate Classroom Photos Registry Translate In The Cloud Google Workspace More on the Cloud Blog Google Cloud See all product updates Android, Chrome & Play Android Chrome Chromebooks Google Play Wear OS Platforms & Devices Fitbit Google Nest Pixel Explore & Get Answers Gemini Maps News Search Shopping Connect & Communicate Classroom Photos Registry Translate In The Cloud Google Workspace More on the Cloud Blog Google Cloud See all product updates Company news Company news Outreach & initiatives Arts & Culture Education Entrepreneurs Public Policy Sustainability See all Technology AI Developers Health Google DeepMind Google Labs Safety and security See all Inside Google Data centers and infrastructure Doodles Googlers Life at Google See all Around the globe Google in Asia Google in Europe Google in Latin America See all Authors Sundar Pichai, CEO Ruth Porat, President & Chief Investment Officer Kent Walker, SVP James Manyika, SVP See all Outreach & initiatives Arts & Culture Education Entrepreneurs Public Policy Sustainability See all * Technology AI Developers Health Google DeepMind Google Labs Safety and security See all * Inside Google Data centers and infrastructure Doodles Googlers Life at Google See all * Around the globe Google in Asia Google in Europe Google in Latin America See all * Authors Sundar Pichai, CEO Ruth Porat, President & Chief Investment Officer Kent Walker, SVP James Manyika, SVP See all Feed Subscribe Subscribe The Keyword Home Product news Product news Android, Chrome & Play Android Chrome Chromebooks Google Play Wear OS Platforms & Devices Fitbit Google Nest Pixel Explore & Get Answers Gemini Maps News Search Shopping Connect & Communicate Classroom Photos Registry Translate In The Cloud Google Workspace More on the Cloud Blog Google Cloud See all product updates Company news Company news Outreach & initiatives Arts & Culture Education Entrepreneurs Public Policy Sustainability See all Technology AI Developers Health Google DeepMind Google Labs Safety and security See all Inside Google Data centers and infrastructure Doodles Googlers Life at Google See all Around the globe Google in Asia Google in Europe Google in Latin America See all Authors Sundar Pichai, CEO Ruth Porat, President & Chief Investment Officer Kent Walker, SVP James Manyika, SVP See all Feed Press corner RSS feed Subscribe Breadcrumb Technology Google Labs State-of-the-art video and image generation with Veo 2 and Imagen 3 Dec 16, 2024 ·[[read-time]] min read Share TwitterFacebookLinkedInMail Copy link We’re announcing new versions of Veo and Imagen, and introducing our latest experiment in image generation: Whisk. Aäron van den Oord Research Scientist, Google DeepMind Elias Roman Senior Director, Product Management, Google Labs Share TwitterFacebookLinkedInMail Copy link Earlier this year, we introduced our video generation model, Veo, and our latest image generation model, Imagen 3. Since then, it’s been exciting to watch people bring their ideas to life with help from these models: YouTube creators are exploring the creative possibilities of video backgrounds for their YouTube Shorts, enterprise customers are enhancing creative workflows on Vertex AI and creatives are using VideoFX and ImageFX to tell their stories. Together with collaborators ranging from filmmakers to businesses, we’re continuing to develop and evolve these technologies. Today we're introducing a new video model, Veo 2, and the latest version of Imagen 3, both of which achieve state-of-the-art results. These models are now available in VideoFX, ImageFX and our newest Labs experiment, Whisk. Veo 2: state-of-the-art video generation Veo 2 creates incredibly high-quality videos in a wide range of subjects and styles. In head-to-head comparisons judged by human raters, Veo 2 achieved state-of-the-art results against leading models. It brings an improved understanding of real-world physics and the nuances of human movement and expression, which helps improve its detail and realism overall. Veo 2 understands the unique language of cinematography: ask it for a genre, specify a lens, suggest cinematic effects and Veo 2 will deliver — at resolutions up to 4K, and extended to minutes in length. Ask for a low-angle tracking shot that glides through the middle of a scene, or a close-up shot on the face of a scientist looking through her microscope, and Veo 2 creates it. Suggest “18mm lens” in your prompt and Veo 2 knows to craft the wide angle shot that this lens is known for, or blur out the background and focus on your subject by putting \"shallow depth of field\" in your prompt. Video format not supported Cinematic shot of a female doctor in a dark yellow hazmat suit, illuminated by the harsh fluorescent light of a laboratory. The camera slowly zooms in on her face, panning gently to emphasize the worry and anxiety etched across her brow. She is hunched over a lab table, peering intently into a microscope, her gloved hands carefully adjusting the focus. The muted color palette of the scene, dominated by the sickly yellow of the suit and the sterile steel of the lab, underscores the gravity of the situation and the weight of the unknown she is facing. The shallow depth of field focuses on the fear in her eyes, reflecting the immense pressure and responsibility she bears. Examples of Veo 2's high-quality video generation capabilities. All videos were generated by Veo 2 and have not been modified. Video format not supported This medium shot, with a shallow depth of field, portrays an adorable cartoon girl with wavy brown hair and lots of character, sitting upright in a 1980s kitchen. Her hair is medium length and wavy. She has a small, slightly upturned nose, and small, rounded ears. She is very animated and excited as she talks to the camera and lighting and giggling with a huge grin. Examples of Veo 2's high-quality video generation capabilities. All videos were generated by Veo 2 and have not been modified. Video format not supported The camera floats gently through rows of pastel-painted wooden beehives, buzzing honeybees gliding in and out of frame. The motion settles on the refined farmer standing at the center, his pristine white beekeeping suit gleaming in the golden afternoon light. He lifts a jar of honey, tilting it slightly to catch the light. Behind him, tall sunflowers sway rhythmically in the breeze, their petals glowing in the warm sunlight. The camera tilts upward to reveal a retro farmhouse with mint-green shutters, its walls dappled with shadows from swaying trees. Shot with a 35mm lens on Kodak Portra 400 film, the golden light creates rich textures on the farmer’s gloves, marmalade jar, and weathered wood of the beehives. Examples of Veo 2's high-quality video generation capabilities. All videos were generated by Veo 2 and have not been modified. Video format not supported A low-angle shot captures a flock of pink flamingos gracefully wading in a lush, tranquil lagoon. The vibrant pink of their plumage contrasts beautifully with the verdant green of the surrounding vegetation and the crystal-clear turquoise water. Sunlight glints off the water's surface, creating shimmering reflections that dance on the flamingos' feathers. The birds' elegant, curved necks are submerged as they walk through the shallow water, their movements creating gentle ripples that spread across the lagoon. The composition emphasizes the serenity and natural beauty of the scene, highlighting the delicate balance of the ecosystem and the inherent grace of these magnificent birds. The soft, diffused light of early morning bathes the entire scene in a warm, ethereal glow. Examples of Veo 2's high-quality video generation capabilities. All videos were generated by Veo 2 and have not been modified. Video format not supported A perfect cube rotates in the center of a soft, foggy void. The surface shifts between different hyper-real textures—smooth marble, velvety suede, hammered brass, and raw concrete. Each material reveals subtle details: marble veins slowly spreading, suede fibers brushing with wind, brass tarnishing in slow motion, and concrete crumbling to reveal polished stone inside. Ends with a soft glow surrounding the cube as it transitions to a smooth mirrored surface, reflecting infinity. Examples of Veo 2's high-quality video generation capabilities. All videos were generated by Veo 2 and have not been modified. Video format not supported A cinematic shot captures a fluffy Cockapoo, perched atop a vibrant pink flamingo float, in a sun-drenched Los Angeles swimming pool. The crystal-clear water sparkles under the bright California sun, reflecting the playful scene. The Cockapoo's fur, a soft blend of white and apricot, is highlighted by the golden sunlight, its floppy ears gently swaying in the breeze. Its happy expression and wagging tail convey pure joy and summer bliss. The vibrant pink flamingo adds a whimsical touch, creating a picture-perfect image of carefree fun in the LA sunshine. Examples of Veo 2's high-quality video generation capabilities. All videos were generated by Veo 2 and have not been modified. Video format not supported The sun rises slowly behind a perfectly plated breakfast scene. Thick, golden maple syrup pours in slow motion over a stack of fluffy pancakes, each one releasing a soft, warm steam cloud. A close-up of crispy bacon sizzles, sending tiny embers of golden grease into the air. Coffee pours in smooth, swirling motion into a crystal-clear cup, filling it with deep brown layers of crema. Scene ends with a camera swoop into a fresh-cut orange, revealing its bright, juicy segments in stunning macro detail. Examples of Veo 2's high-quality video generation capabilities. All videos were generated by Veo 2 and have not been modified. 1 2 3 4 5 6 7 While video models often “hallucinate” unwanted details — extra fingers or unexpected objects, for example — Veo 2 produces these less frequently, making outputs more realistic. Our commitment to safety and responsible development has guided Veo 2. We have been intentionally measured in growing Veo’s availability, so we can help identify, understand and improve the model’s quality and safety while slowly rolling it out via VideoFX, YouTube and Vertex AI. Just like the rest of our image and video generation models, Veo 2 outputs include an invisible SynthID watermark that helps identify them as AI-generated, helping reduce the chances of misinformation and misattribution. Today, we're bringing our new Veo 2 capabilities to our Google Labs video generation tool, VideoFX, and expanding the number of users who can access it. Visit Google Labs to sign up for the waitlist. We also plan to expand Veo 2 to YouTube Shorts and other products next year. Note: Find prompts for all videos at the bottom of this post: Scientist1, Cartoon character2, Bees3, Flamingos4, Cube5, Dog6, Pancakes7 Imagen 3: state-of-the-art image generation We've also improved our Imagen 3 image-generation model, which now generates brighter, better composed images. It can now render more diverse art styles with greater accuracy — from photorealism to impressionism, from abstract to anime. This upgrade also follows prompts more faithfully, and renders richer details and textures. In side-by-side comparisons of outputs by human raters against leading image generation models, Imagen 3 achieved state-of-the-art results. Starting today, the latest Imagen 3 model will globally roll out in ImageFX, our image generation tool from Google Labs, to more than 100 countries. Visit ImageFX to get started. Examples of Imagen 3's rich detail and image quality composition Examples of Imagen 3's rich detail and image quality composition Examples of Imagen 3's rich detail and image quality composition Examples of Imagen 3's rich detail and image quality composition Examples of Imagen 3's rich detail and image quality composition 1 2 3 4 5 Note: Find prompts for all images at the bottom of this post: Potter8, Squirrel9, Train station10, Woman11, Strawberry bird12 Whisk: a fun new tool that lets you prompt with images to visualize your ideas Whisk, our newest experiment from Google Labs, lets you input or create images that convey the subject, scene and style you have in mind. Then, you can bring them together and remix them to create something uniquely your own, from a digital plushie to an enamel pin or sticker. Under the hood, Whisk combines our latest Imagen 3 model with Gemini’s visual understanding and description capabilities. The Gemini model automatically writes a detailed caption of your images, and it then feeds those descriptions into Imagen 3. This process allows you to easily remix your subjects, scenes and styles in fun, new ways. 10:25 Whisk lets you quickly visualize and remix ideas. Whisk is launching in the U.S. today. Read more about Whisk and try it out at labs.google/Whisk. POSTED IN: Google Labs Google DeepMind Read Article More Information 1 Veo 2 prompt: Cinematic shot of a female doctor in a dark yellow hazmat suit, illuminated by the harsh fluorescent light of a laboratory. The camera slowly zooms in on her face, panning gently to emphasize the worry and anxiety etched across her brow. She is hunched over a lab table, peering intently into a microscope, her gloved hands carefully adjusting the focus. The muted color palette of the scene, dominated by the sickly yellow of the suit and the sterile steel of the lab, underscores the gravity of the situation and the weight of the unknown she is facing. The shallow depth of field focuses on the fear in her eyes, reflecting the immense pressure and responsibility she bears. 2 Veo 2 prompt: This medium shot, with a shallow depth of field, portrays an adorable cartoon girl with wavy brown hair and lots of character, sitting upright in a 1980s kitchen. Her hair is medium length and wavy. She has a small, slightly upturned nose, and small, rounded ears. She is very animated and excited as she talks to the camera and lighting and giggling with a huge grin. 3 Veo 2 prompt: The camera floats gently through rows of pastel-painted wooden beehives, buzzing honeybees gliding in and out of frame. The motion settles on the refined farmer standing at the center, his pristine white beekeeping suit gleaming in the golden afternoon light. He lifts a jar of honey, tilting it slightly to catch the light. Behind him, tall sunflowers sway rhythmically in the breeze, their petals glowing in the warm sunlight. The camera tilts upward to reveal a retro farmhouse with mint-green shutters, its walls dappled with shadows from swaying trees. Shot with a 35mm lens on Kodak Portra 400 film, the golden light creates rich textures on the farmer’s gloves, marmalade jar, and weathered wood of the beehives. 4 Veo 2 prompt: A low-angle shot captures a flock of pink flamingos gracefully wading in a lush, tranquil lagoon. The vibrant pink of their plumage contrasts beautifully with the verdant green of the surrounding vegetation and the crystal-clear turquoise water. Sunlight glints off the water's surface, creating shimmering reflections that dance on the flamingos' feathers. The birds' elegant, curved necks are submerged as they walk through the shallow water, their movements creating gentle ripples that spread across the lagoon. The composition emphasizes the serenity and natural beauty of the scene, highlighting the delicate balance of the ecosystem and the inherent grace of these magnificent birds. The soft, diffused light of early morning bathes the entire scene in a warm, ethereal glow. 5 Veo 2 prompt: A perfect cube rotates in the center of a soft, foggy void. The surface shifts between different hyper-real textures—smooth marble, velvety suede, hammered brass, and raw concrete. Each material reveals subtle details: marble veins slowly spreading, suede fibers brushing with wind, brass tarnishing in slow motion, and concrete crumbling to reveal polished stone inside. Ends with a soft glow surrounding the cube as it transitions to a smooth mirrored surface, reflecting infinity. 6 Veo 2 prompt: A cinematic shot captures a fluffy Cockapoo, perched atop a vibrant pink flamingo float, in a sun-drenched Los Angeles swimming pool. The crystal-clear water sparkles under the bright California sun, reflecting the playful scene. The Cockapoo's fur, a soft blend of white and apricot, is highlighted by the golden sunlight, its floppy ears gently swaying in the breeze. Its happy expression and wagging tail convey pure joy and summer bliss. The vibrant pink flamingo adds a whimsical touch, creating a picture-perfect image of carefree fun in the LA sunshine. 7 Veo 2 prompt: The sun rises slowly behind a perfectly plated breakfast scene. Thick, golden maple syrup pours in slow motion over a stack of fluffy pancakes, each one releasing a soft, warm steam cloud. A close-up of crispy bacon sizzles, sending tiny embers of golden grease into the air. Coffee pours in smooth, swirling motion into a crystal-clear cup, filling it with deep brown layers of crema. Scene ends with a camera swoop into a fresh-cut orange, revealing its bright, juicy segments in stunning macro detail. 8 Imagen 3 prompt: An extreme close-up of a craftsperson's hands shaping a glowing piece of pottery on a wheel. Threads of golden, luminous energy connect the potter’s hands to the clay, swirling dynamically with their movements. The workspace is filled with rich textures—dusty shelves lined with tools, scattered clay fragments, and beams of natural light piercing through wooden shutters. The interplay of light and energy creates an ethereal, almost magical atmosphere 9 Imagen 3 prompt: A close-up shot captures a winter wonderland scene – soft snowflakes fall on a snow-covered forest floor. Behind a frosted pine branch, a red squirrel sits, its bright orange fur a splash of color against the white. It holds a small hazelnut. As it enjoys its meal, it seems oblivious to the falling snow. 10 Imagen 3 prompt: A foggy 1940s European train station at dawn, framed by intricate wrought-iron arches and misted glass windows. Steam rises from the tracks, blending with dense fog. Two lovers stand in an emotional embrace near the train, backlit by the warm, amber glow of dim lanterns. The departing train is partially visible, its red tail lights fading into the mist. The woman wears a faded red coat and clutches a small leather diary, while the man is dressed in a weathered soldier’s uniform. Dust motes float in the air, illuminated by the soft golden backlight. The atmosphere is melancholic and timeless, evoking the bittersweet farewell of wartime cinema. 11 Imagen 3 prompt: A portrait of an Asian woman with neon green lights in the background, shallow depth of field. 12 Imagen 3 prompt: A close-up, macro photography stock photo of a strawberry intricately sculpted into the shape of a hummingbird in mid-flight, its wings a blur as it sips nectar from a vibrant, tubular flower. The backdrop features a lush, colorful garden with a soft, bokeh effect, creating a dreamlike atmosphere. The image is exceptionally detailed and captured with a shallow depth of field, ensuring a razor-sharp focus on the strawberry-hummingbird and gentle fading of the background. The high resolution, professional photographers style, and soft lighting illuminate the scene in a very detailed manner, professional color grading amplifies the vibrant colors and creates an image with exceptional clarity. The depth of field makes the hummingbird and flower stand out starkly against the bokeh background. Collapse Related stories Google Labs #### Whisk: Visualize and remix ideas using images and AI By Thomas Iljic & Nicole Brichtova Dec 16, 2024 Google Labs #### NotebookLM gets a new look, audio interactivity and a premium version By Steven Johnson Dec 13, 2024 Google DeepMind #### Introducing Gemini 2.0: our new AI model for the agentic era By Sundar Pichai & Demis Hassabis & Koray Kavukcuoglu Dec 11, 2024 Google Labs #### Listen to your first-ever 2024 Spotify Wrapped AI podcast, built with NotebookLM By Janakitti Ratana-Rueangsri Dec 04, 2024 Google DeepMind #### AlphaQubit tackles one of quantum computing’s biggest challenges By Google DeepMind and Quantum AI teams Nov 20, 2024 AI #### How songwriter Justin Tranter helped evolve Music AI Sandbox By Arathi Sethumadhavan Nov 19, 2024 . Let’s stay in touch. Get the latest news from Google in your inbox. Subscribe No thanks Follow Us Privacy Terms About Google Google Products About the Keyword Help *",
      "title": "https://www.marktechpost.com/2024/12/17/google-released-state-of-the-art-veo-2-for-video-generation-and-improved-imagen-3-for-image-creation-setting-new-standards-with-4k-video-and-several-minutes-long-video-generation/"
    },
    {
      "url": "https://neurohive.io/en/state-of-the-art/step-video-t2v-text-to-video-open-source-model-achieves-16x-video-compression-breakthrough/",
      "content": "A Dive into Text-to-Video Models Hugging Face Models Datasets Spaces Posts Docs Enterprise Pricing Log In Sign Up Back to Articles Text-to-Video: The Task, Challenges and the Current State Published May 8, 2023 Update on GitHub Upvote 30 +24 adirik Alara Dirik Video samples generated with ModelScope. Text-to-Video vs. Text-to-Image How to Generate Videos from Text? Datasets Text-to-Video at Hugging Face Hugging Face Demos Community Contributions and Open Source Text-to-Video Projects Conclusion Text-to-video is next in line in the long list of incredible advances in generative models. As self-descriptive as it is, text-to-video is a fairly new computer vision task that involves generating a sequence of images from text descriptions that are both temporally and spatially consistent. While this task might seem extremely similar to text-to-image, it is notoriously more difficult. How do these models work, how do they differ from text-to-image models, and what kind of performance can we expect from them? In this blog post, we will discuss the past, present, and future of text-to-video models. We will start by reviewing the differences between the text-to-video and text-to-image tasks, and discuss the unique challenges of unconditional and text-conditioned video generation. Additionally, we will cover the most recent developments in text-to-video models, exploring how these methods work and what they are capable of. Finally, we will talk about what we are working on at Hugging Face to facilitate the integration and use of these models and share some cool demos and resources both on and outside of the Hugging Face Hub. Examples of videos generated from various text description inputs, image taken from Make-a-Video. Text-to-Video vs. Text-to-Image With so many recent developments, it can be difficult to keep up with the current state of text-to-image generative models. Let's do a quick recap first. Just two years ago, the first open-vocabulary, high-quality text-to-image generative models emerged. This first wave of text-to-image models, including VQGAN-CLIP, XMC-GAN, and GauGAN2, all had GAN architectures. These were quickly followed by OpenAI's massively popular transformer-based DALL-E in early 2021, DALL-E 2 in April 2022, and a new wave of diffusion models pioneered by Stable Diffusion and Imagen. The huge success of Stable Diffusion led to many productionized diffusion models, such as DreamStudio and RunwayML GEN-1, and integration with existing products, such as Midjourney. Despite the impressive capabilities of diffusion models in text-to-image generation, diffusion and non-diffusion based text-to-video models are significantly more limited in their generative capabilities. Text-to-video are typically trained on very short clips, meaning they require a computationally expensive and slow sliding window approach to generate long videos. As a result, these models are notoriously difficult to deploy and scale and remain limited in context and length. The text-to-video task faces unique challenges on multiple fronts. Some of these main challenges include: Computational challenges: Ensuring spatial and temporal consistency across frames creates long-term dependencies that come with a high computation cost, making training such models unaffordable for most researchers. Lack of high-quality datasets: Multi-modal datasets for text-to-video generation are scarce and often sparsely annotated, making it difficult to learn complex movement semantics. Vagueness around video captioning: Describing videos in a way that makes them easier for models to learn from is an open question. More than a single short text prompt is required to provide a complete video description. A generated video must be conditioned on a sequence of prompts or a story that narrates what happens over time. In the next section, we will discuss the timeline of developments in the text-to-video domain and the various methods proposed to address these challenges separately. On a higher level, text-to-video works propose one of these: New, higher-quality datasets that are easier to learn from. Methods to train such models without paired text-video data. More computationally efficient methods to generate longer and higher resolution videos. How to Generate Videos from Text? Let's take a look at how text-to-video generation works and the latest developments in this field. We will explore how text-to-video models have evolved, following a similar path to text-to-image research, and how the specific challenges of text-to-video generation have been tackled so far. Like the text-to-image task, early work on text-to-video generation dates back only a few years. Early research predominantly used GAN and VAE-based approaches to auto-regressively generate frames given a caption (see Text2Filter and TGANs-C). While these works provided the foundation for a new computer vision task, they are limited to low resolutions, short-range, and singular, isolated motions. Initial text-to-video models were extremely limited in resolution, context and length, image taken from TGANs-C. Taking inspiration from the success of large-scale pretrained transformer models in text (GPT-3) and image (DALL-E), the next surge of text-to-video generation research adopted transformer architectures. Phenaki, Make-A-Video, NUWA, VideoGPT and CogVideo all propose transformer-based frameworks, while works such as TATS propose hybrid methods that combine VQGAN for image generation and a time-sensitive transformer module for sequential generation of frames. Out of this second wave of works, Phenaki is particularly interesting as it enables generating arbitrary long videos conditioned on a sequence of prompts, in other words, a story line. Similarly, NUWA-Infinity proposes an autoregressive over autoregressive generation mechanism for infinite image and video synthesis from text inputs, enabling the generation of long, HD quality videos. However, neither Phenaki or NUWA models are publicly available. Phenaki features a transformer-based architecture, image taken from here. The third and current wave of text-to-video models features predominantly diffusion-based architectures. The remarkable success of diffusion models in diverse, hyper-realistic, and contextually rich image generation has led to an interest in generalizing diffusion models to other domains such as audio, 3D, and, more recently, video. This wave of models is pioneered by Video Diffusion Models (VDM), which extend diffusion models to the video domain, and MagicVideo, which proposes a framework to generate video clips in a low-dimensional latent space and reports huge efficiency gains over VDM. Another notable mention is Tune-a-Video, which fine-tunes a pretrained text-to-image model with a single text-video pair and enables changing the video content while preserving the motion. The continuously expanding list of text-to-video diffusion models that followed include Video LDM, Text2Video-Zero, Runway Gen1 and Gen2, and NUWA-XL. Text2Video-Zero is a text-guided video generation and manipulation framework that works in a fashion similar to ControlNet. It can directly generate (or edit) videos based on text inputs, as well as combined text-pose or text-edge data inputs. As implied by its name, Text2Video-Zero is a zero-shot model that combines a trainable motion dynamics module with a pre-trained text-to-image Stable Diffusion model without using any paired text-video data. Similarly to Text2Video-Zero, Runway’s Gen-1 and Gen-2 models enable synthesizing videos guided by content described through text or images. Most of these works are trained on short video clips and rely on autoregressive generation with a sliding window to generate longer videos, inevitably resulting in a context gap. NUWA-XL addresses this issue and proposes a “diffusion over diffusion” method to train models on 3376 frames. Finally, there are open-source text-to-video models and frameworks such as Alibaba / DAMO Vision Intelligence Lab’s ModelScope and Tencel’s VideoCrafter, which haven't been published in peer-reviewed conferences or journals. Datasets Like other vision-language models, text-to-video models are typically trained on large paired datasets videos and text descriptions. The videos in these datasets are typically split into short, fixed-length chunks and often limited to isolated actions with a few objects. While this is partly due to computational limitations and partly due to the difficulty of describing video content in a meaningful way, we see that developments in multimodal video-text datasets and text-to-video models are often entwined. While some work focuses on developing better, more generalizable datasets that are easier to learn from, works such as Phenaki explore alternative solutions such as combining text-image pairs with text-video pairs for the text-to-video task. Make-a-Video takes this even further by proposing using only text-image pairs to learn what the world looks like and unimodal video data to learn spatio-temporal dependencies in an unsupervised fashion. These large datasets experience similar issues to those found in text-to-image datasets. The most commonly used text-video dataset, WebVid, consists of 10.7 million pairs of text-video pairs (52K video hours) and contains a fair amount of noisy samples with irrelevant video descriptions. Other datasets try to overcome this issue by focusing on specific tasks or domains. For example, the Howto100M dataset consists of 136M video clips with captions that describe how to perform complex tasks such as cooking, handcrafting, gardening, and fitness step-by-step. Similarly, the QuerYD dataset focuses on the event localization task such that the captions of videos describe the relative location of objects and actions in detail. CelebV-Text is a large-scale facial text-video dataset of over 70K videos to generate videos with realistic faces, emotions, and gestures. Text-to-Video at Hugging Face Using Hugging Face Diffusers, you can easily download, run and fine-tune various pretrained text-to-video models, including Text2Video-Zero and ModelScope by Alibaba / DAMO Vision Intelligence Lab. We are currently working on integrating other exciting works into Diffusers and 🤗 Transformers. Hugging Face Demos At Hugging Face, our goal is to make it easier to use and build upon state-of-the-art research. Head over to our hub to see and play around with Spaces demos contributed by the 🤗 team, countless community contributors and research authors. At the moment, we host demos for VideoGPT, CogVideo, ModelScope Text-to-Video, and Text2Video-Zero with many more to come. To see what we can do with these models, let's take a look at the Text2Video-Zero demo. This demo not only illustrates text-to-video generation but also enables multiple other generation modes for text-guided video editing and joint conditional video generation using pose, depth and edge inputs along with text prompts. Apart from using demos to experiment with pretrained text-to-video models, you can also use the Tune-a-Video training demo to fine-tune an existing text-to-image model with your own text-video pair. To try it out, upload a video and enter a text prompt that describes the video. Once the training is done, you can upload it to the Hub under the Tune-a-Video community or your own username, publicly or privately. Once the training is done, simply head over to the Run tab of the demo to generate videos from any text prompt. All Spaces on the 🤗 Hub are Git repos you can clone and run on your local or deployment environment. Let’s clone the ModelScope demo, install the requirements, and run it locally. git clone https://huggingface.co/spaces/damo-vilab/modelscope-text-to-video-synthesis cd modelscope-text-to-video-synthesis pip install -r requirements.txt python app.py And that's it! The Modelscope demo is now running locally on your computer. Note that the ModelScope text-to-video model is supported in Diffusers and you can directly load and use the model to generate new videos with a few lines of code. ``` import torch from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler from diffusers.utils import export_to_video pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\") pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config) pipe.enable_model_cpu_offload() prompt = \"Spiderman is surfing\" video_frames = pipe(prompt, num_inference_steps=25).frames video_path = export_to_video(video_frames) ``` Community Contributions and Open Source Text-to-Video Projects Finally, there are various open source projects and models that are not on the hub. Some notable mentions are Phil Wang’s (aka lucidrains) unofficial implementations of Imagen, Phenaki, NUWA, Make-a-Video and Video Diffusion Models. Another exciting project by ExponentialML builds on top of 🤗 diffusers to finetune ModelScope Text-to-Video. Conclusion Text-to-video research is progressing exponentially, but existing work is still limited in context and faces many challenges. In this blog post, we covered the constraints, unique challenges and the current state of text-to-video generation models. We also saw how architectural paradigms originally designed for other tasks enable giant leaps in the text-to-video generation task and what this means for future research. While the developments are impressive, text-to-video models still have a long way to go compared to text-to-image models. Finally, we also showed how you can use these models to perform various tasks using the demos available on the Hub or as a part of 🤗 Diffusers pipelines. That was it! We are continuing to integrate the most impactful computer vision and multi-modal models and would love to hear back from you. To stay up to date with the latest news in computer vision and multi-modal research, you can follow us on Twitter: @adirik, @a_e_roberts, @osanseviero, @risingsayak and @huggingface. More Articles from our Blog Practical 3D Asset Generation: A Step-by-Step Guide --------------------------------------------------- By dylanebert August 1, 2023 • 6 Stable Diffusion in JAX/Flax 🚀 ------------------------------- By pcuenca October 13, 2022 • 2 Community buthjjn22 days ago A man in an aeroplane + Reply Dammy11217 days ago This comment has been hidden + EditPreview Upload images, audio, and videos by dragging in the text input, pasting, or clicking here. Tap or paste here to upload images Comment· Sign up or log in to comment Upvote 30 +18 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs",
      "title": "https://neurohive.io/en/state-of-the-art/step-video-t2v-text-to-video-open-source-model-achieves-16x-video-compression-breakthrough/"
    },
    {
      "url": "https://blog.google/technology/ai/2024-ai-extraordinary-progress-advancement/",
      "content": "Published Time: 2025-01-23T16:00:00+00:00 Year in review: Google's biggest AI advancements of 2024 [{\"model\": \"blogsurvey.survey\", \"pk\": 5, \"fields\": {\"name\": \"Sentiment Change - All Articles - Nov 2024\", \"survey_id\": \"sentiment-change-all-articles-nov-2024_241031\", \"scroll_depth_trigger\": 75, \"previous_survey\": null, \"display_rate\": 50, \"thank_message\": \"Thank you!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"c32a4772-5575-4985-814a-afd8d15d5d6d\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {\\\"question\\\": \\\"After reading this article, how has your perception of Google changed?\\\", \\\"responses\\\": [{\\\"id\\\": \\\"cb111cda-60ba-4ac5-8260-17c5326e485b\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Gotten better\\\"}, {\\\"id\\\": \\\"d8864abb-689a-4b52-b021-449af0b0a7c6\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Gotten worse\\\"}, {\\\"id\\\": \\\"f169d98d-1731-4efc-be90-3cd379a2a63e\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Stayed the same\\\"}, {\\\"id\\\": \\\"701d8c63-affe-4f44-85f0-71538a310d65\\\", \\\"type\\\": \\\"item\\\", \\\"value\\\": \\\"Don't know\\\"}]}}]\", \"target_article_pages\": true}}] Skip to main content The Keyword 2024: A year of extraordinary progress and advancement in AI Share TwitterFacebookLinkedInMail Copy link Home Product news Product news Android, Chrome & Play Android Chrome Chromebooks Google Play Wear OS See all Platforms & Devices Fitbit Google Nest Pixel See all Explore & Get Answers Gemini Maps News Search Shopping See all Connect & Communicate Classroom Photos Registry Translate In the Cloud Google Workspace More on the Cloud Blog Google Cloud See all See all product updates Android, Chrome & Play Android Chrome Chromebooks Google Play Wear OS See all * Platforms & Devices Fitbit Google Nest Pixel See all * Explore & Get Answers Gemini Maps News Search Shopping See all * Connect & Communicate * Classroom * Photos * Registry * Translate * In the Cloud Google Workspace More on the Cloud Blog Google Cloud See all See all product updates Company news Company news Outreach & initiatives Arts & Culture Education Entrepreneurs Public Policy Sustainability See all Technology AI Developers Health Google DeepMind Google Labs Safety and security See all Inside Google Data centers and infrastructure Doodles Googlers Life at Google See all Around the globe Google in Asia Google in Europe Google in Latin America See all Authors Sundar Pichai, CEO Ruth Porat, President & Chief Investment Officer Kent Walker, SVP James Manyika, SVP See all Outreach & initiatives Arts & Culture Education Entrepreneurs Public Policy Sustainability See all * Technology AI Developers Health Google DeepMind Google Labs Safety and security See all * Inside Google Data centers and infrastructure Doodles Googlers Life at Google See all * Around the globe Google in Asia Google in Europe Google in Latin America See all * Authors Sundar Pichai, CEO Ruth Porat, President & Chief Investment Officer Kent Walker, SVP James Manyika, SVP See all Feed Subscribe Subscribe The Keyword Home Product news Product news Android, Chrome & Play Android Chrome Chromebooks Google Play Wear OS See all Platforms & Devices Fitbit Google Nest Pixel See all Explore & Get Answers Gemini Maps News Search Shopping See all Connect & Communicate Classroom Photos Registry Translate In the Cloud Google Workspace More on the Cloud Blog Google Cloud See all See all product updates Company news Company news Outreach & initiatives Arts & Culture Education Entrepreneurs Public Policy Sustainability See all Technology AI Developers Health Google DeepMind Google Labs Safety and security See all Inside Google Data centers and infrastructure Doodles Googlers Life at Google See all Around the globe Google in Asia Google in Europe Google in Latin America See all Authors Sundar Pichai, CEO Ruth Porat, President & Chief Investment Officer Kent Walker, SVP James Manyika, SVP See all Feed Press corner RSS feed Subscribe Breadcrumb Technology AI 2024: A year of extraordinary progress and advancement in AI Jan 23, 2025 ·[[read-time]] min read Share TwitterFacebookLinkedInMail Copy link A look back on a year of breakthroughs, progress and extraordinary accomplishments. Demis Hassabis CEO Google DeepMind James Manyika Senior Vice President, Research, Technology & Society Jeff Dean Chief Scientist Read AI-generated summary Bullet points This article summarizes Google's AI advancements in 2024, highlighting their commitment to responsible development. Google released Gemini 2.0, a powerful AI model designed for the \"agentic era,\" and integrated it into various products. They made significant progress in generative AI, releasing updates to Imagen, Veo, and MusicFX, empowering creativity. Google also advanced robotics, hardware, and computing, with breakthroughs in quantum computing and chip design. They explored AI's potential in science, biology, and mathematics, with notable achievements in protein structure prediction and geometry. Summaries were generated by Google AI. Generative AI is experimental. Share TwitterFacebookLinkedInMail Copy link As we move into 2025, we wanted to take a moment to recognize the astonishing progress of the last year. From new Gemini models built for the agentic era and empowering creativity, to an AI system that designs novel, high-strength protein binders, AI–enabled neuroscience and even landmark advances in quantum computing, we’ve been boldly and responsibly advancing the frontiers of artificial intelligence and all the ways it can benefit humanity. As we and our colleagues wrote two years ago in an essay titled Why we focus on AI: “Our approach to developing and harnessing the potential of AI is grounded in our founding mission — to organize the world’s information and make it universally accessible and useful — and it is shaped by our commitment to improve the lives of as many people as possible.” This remains as true today as it was when we first wrote it. In this 2024 Year-in-Review post, we look back on a year's worth of extraordinary progress in AI, made possible by the many incredible teams across Google, that helped deliver on that mission and commitment — progress that sets the stage for more to come this year. Relentless innovation in models, products and technologies 2024 was a year of experimenting, fast shipping, and putting our latest technologies in the hands of developers. In December 2024, we released the first models in our Gemini 2.0 experimental series — AI models designed for the agentic era. First out of the gate was Gemini 2.0 Flash, our workhorse model, followed by prototypes from the frontiers of our agentic research including: an updated Project Astra, which explores the capabilities of a universal AI assistant; Project Mariner, an early prototype capable of taking actions in Chrome as an experimental extension; and Jules, an AI-powered code agent. We're looking forward to bringing Gemini 2.0’s powerful capabilities to our flagship products — in Search, we’ve already started testing in AI Overviews, which are now used by over a billion people to ask new types of questions. We also released Deep Research, a new agentic feature in Gemini Advanced that saves people hours of research work by creating and executing multi-step plans for finding answers to complicated questions; and introduced Gemini 2.0 Flash Thinking Experimental, an experimental model that explicitly shows its thoughts. These advances followed swift progress earlier in the year, from incorporating Gemini’s capabilities into more Google products to the release of Gemini 1.5 Pro and Gemini 1.5 Flash — a model optimized for speed and efficiency. 1.5 Flash’s compact size made it more cost-efficient to serve, and in 2024 it became our most popular model for developers. And we improved and updated AI Studio, which provides a host of resources for developers. It is now available as a progressive web app (PWA) that can be installed on desktop, iOS and Android. Notably, it’s been exciting to see the public reception to several new features for NotebookLM, such as Audio Overviews, which can take uploaded source material and produce a “deep dive” discussion between two AI hosts. Your browser does not support the audio element. NotebookLM Audio OverviewIn this Audio Overview, two AI hosts dive into the world of NotebookLM updates. More natural and intuitive handling of speech input and output remains at the core of several of our products: Gemini Live, Project Astra, Journey Voices and YouTube’s auto dubbing. Continuing our long history of contributing innovations to the open community — such as with Transformers, TensorFlow, BERT, T5, JAX, AlphaFold and AlphaCode — we released two new models from Gemma, our state-of-the-art open model built from the same research and technology used to create the Gemini models. Gemma outperformed similarly sized open models on capabilities like question answering, reasoning, math / science and coding. And we released Gemma Scope, which provides tools that help researchers understand the inner workings of Gemma 2. We also continued to improve the factuality of our models and minimize hallucinations. In December, for example, we published FACTS Grounding, a new benchmark — based on collaboration between Google DeepMind, Google Research and Kaggle — for evaluating how accurately large language models ground their responses in provided source material and avoid hallucinations. The FACTS Grounding dataset comprises 1,719 examples, each carefully crafted to require long-form responses grounded in the context document provided. We tested leading LLMs using FACTS Grounding, launched the FACTS leaderboard on Kaggle and are proud that Gemini 2.0 Flash Experimental, Gemini 1.5 Flash and Gemini 1.5 Pro currently have the three highest factuality scores, with gemini-2.0-flash-exp at 83.6%. Moreover, we improved underlying ML efficiency through pioneering techniques like blockwise parallel decoding, improved confidence-based deferral and speculative decoding that reduce the inference times of LLMs, allowing them to generate responses more quickly. These improvements are used across Google products and set a standard throughout the industry. Combining AI with sport, in March we released TacticAI, an AI system for football tactics that can provide experts with tactical insights, particularly on corner kicks. Underlying all of our models and products is our ongoing commitment to research leadership. Indeed, in a 2010-2023 WIPO survey of citations for papers on Generative AI, Google including Google Research and Google DeepMind’s citations were more than double the second-most cited institution. This WIPO graph, based on January 2024 data from The Lens, illustrates more than a decade’s worth of Alphabet’s generative AI scientific publication efforts. Finally, progress was made with Project Starline, our “magic window” technology project that enables friends, families and coworkers to feel like they’re together from any distance. We partnered with HP to start commercialization, with the goal of enabling it directly from video conferencing services like Google Meet and Zoom. Empowering creative vision with generative AI We believe AI holds great potential to enable new forms of creativity, democratize creative output and help people express their artistic visions. This is why last year we introduced a series of updates across our generative media tools, covering images, music and video. At the start of 2024, we introduced ImageFX, a new generative AI tool that creates images from text prompts, and MusicFX, a tool for creating up-to-70-second audio clips also based on text prompts. At I/O, we shared an early preview of MusicFX DJ, a tool that helps bring the joy of live music creation to more people. In October, we collaborated with Jacob Collier on making MusicFX DJ simpler to use, especially for new or aspiring musicians. And we updated our music AI toolkit Music AI Sandbox, and evolved our Dream Track experiment which allowed U.S. creators to explore a range of genres and prompts that generate instrumental soundtracks with powerful text-to-music models. Later in 2024, we released state-of-the-art updates to our image and video models: Veo 2 and Imagen 3. As our highest quality text-to-image model, Imagen 3 is capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models; while Veo demonstrated an improved understanding of real-world physics and the nuances of human movement and expression alongside its overall attention-to-detail and realism. Veo represents a significant step forward in high-quality video generation. Research in this field continued apace. We explored ways to use AI to improve editing, for example by using it to control of attributes like transparency, roughness or other physical properties of objects: In these examples of AI editing with synthetic data generation, Input shows a novel, held-out image the model has never seen before. Output shows the model output, which successfully edits material properties. In the field of audio generation, we announced improvements to video-to-audio (V2A) technology, which can generate dynamic soundscapes through natural language text prompts based on on-screen action. This technology is pairable with AI-created video through Veo. Games are an ideal environment for creative exploration of new worlds, as well as training and evaluating embodied agents. In 2024, we introduced Genie 2, a foundation world model capable of generating an endless variety of action-controllable, playable 3D environments for training and evaluating embodied agents. This followed the introduction of SIMA, a Scalable Instructable Multiworld Agent that can follow natural-language instructions to carry out tasks in a variety of video game settings. The architecture of intelligence: advances in robotics, hardware and computing As our multimodal models become more capable and gain a better understanding of the world and its physics, they are making possible incredible new advances in robotics and bringing us closer to our goal of ever-more capable and helpful robots. With ALOHA Unleashed, our robot learned to tie a shoelace, hang a shirt, repair another robot, insert a gear and even clean a kitchen. At the beginning of the year, we introduced AutoRT, SARA-RT and RT-Trajectory, extensions of our Robotics Transformers work intended to help robots better understand and navigate their environments, and make decisions faster. We also published ALOHA Unleashed, a breakthrough in teaching robots on how to use two robotic arms in coordination, and DemoStart, which uses a reinforcement learning algorithm to improve real-world performance on a multi-fingered robotic hand by using simulations. Robotic Transformer 2 (RT-2) is a novel vision-language-action model that learns from both web and robotics data. Beyond robotics, our AlphaChip reinforcement learning method for accelerating and improving chip floorplanning is transforming the design process for chips found in data centers, smartphones and more. To accelerate adoption of these techniques, we released a pre-trained checkpoint to enable external parties to more easily make use of the AlphaChip open source release for their own chip designs. And we made Trillium, our sixth-generation and most performant TPU to date, generally available to Google Cloud customers. Advances in computer chips have accelerated AI. And now, AI can return the favor. AlphaChip can learn the relationships between interconnected chip components and generalize across chips, letting AlphaChip improve with each layout it designs. Our research also focused on correcting the errors in the physical hardware of today's quantum computers. In November, we launched AlphaQubit, an AI-based decoder that identifies quantum computing errors with state-of-the-art accuracy. This collaborative work brought together Google DeepMind’s ML knowledge and Google Research’s error correction expertise to accelerate progress on building a reliable quantum computer. In tests, it made 6% fewer errors than tensor network methods and 30% fewer errors than correlated matching. Then in December, the Google Quantum AI team, part of Google Research, announced Willow, our latest quantum chip which can perform in under five minutes a benchmark computation that would take one of today’s fastest supercomputers 10 septillion years. Willow can reduce errors exponentially as it scales up using more qubits. In fact, it used our quantum error correction to cut the error rate in half, solving a 30+ year challenge known in the field as “below threshold.” This leap forward won the Physics Breakthrough of the Year award. Willow has state-of-the-art performance across a number of metrics. Uncovering new solutions: progress in science, biology and mathematics We continued to push the envelope on accelerating scientific progress with AI-based approaches, releasing a series of tools and papers this year that showed just how useful and powerful a tool AI is for advancing science and mathematics. We're sharing a few highlights. In January, we introduced AlphaGeometry, an AI system engineered to solve complex geometry problems. Our updated version, AlphaGeometry 2, and AlphaProof, a reinforcement-learning-based system for formal math reasoning, achieved the same level as a silver medalist in July 2024’s International Mathematical Olympiad. AlphaGeometry 2 solved Problem 4 in July 2024’s International Mathematical Olympiad within 19 seconds after receiving its formalization. Problem 4 asked to prove the sum of ∠KIL and ∠XPY equals 180°. In collaboration with Isomorphic Labs, we introduced AlphaFold 3, our latest model which predicts the structure and interactions of all of life’s molecules. By accurately predicting the structure of proteins, DNA, RNA, ligands and more, and how they interact, we hope it will transform our understanding of the biological world and drug discovery. AlphaFold 3’s capabilities come from its next-generation architecture and training that now covers all of life’s molecules. We made several key developments in protein-shaping. We announced AlphaProteo, an AI system for designing novel, high-strength protein binders. AlphaProteo can lead to the discovery of new drugs, the development of biosensors and improve our understanding of biological processes. AlphaProteo can generate new protein binders for diverse target proteins. In collaboration with Harvard’s Lichtman Lab and others, we produced a nano-scale mapping of a piece of the human brain at a level of detail never previously achieved, and made it publicly available for researchers to build on. This follows a decade of working to advance our understanding of connectomics, with earlier work on fly brain and mouse brain connectomics now giving way to the larger scale and more complex human brain connectomics. In the deepest layer of the cortex, clusters of cells tend to occur in mirror-image orientation to one another, as shown in this brain mapping project. Then in late November, as part of a broader effort to expand and deepen public dialogue around science and AI, we co-hosted the AI for Science Forum with the Royal Society, which convened scientists, researchers, governmental leaders and executives to discuss key topics like cracking the protein structure prediction challenge, mapping the human brain and saving lives through accurate forecasting and spotting wildfires. We hosted a Q&A with the four Nobel Laureates in attendance at the forum, Sir Paul Nurse, Jennifer Doudna, Demis Hassabis and John Jumper, which is available to listen to via the Google DeepMind podcast. This was also a landmark year for another reason: Demis Hassabis and John Jumper, along with David Baker, were awarded the 2024 Nobel Prize® in Chemistry for their work on AlphaFold 2. As the Nobel committee recognized, their work: \"[H]as opened up completely new possibilities to design proteins that have never been seen before, and we now have access to predicted structures of all 200 million known proteins. These are truly great achievements.\" It was also exciting to see the 2024 Nobel Prize® in Physics awarded to recently retired long-time Googler Geoffrey Hinton (along with John Hopfield), \"for foundational discoveries and inventions that enable machine learning with artificial neural networks.” The Nobels followed additional recognitions for Google including the NeurIPS 2024 Test of Time Paper Awards for Sequence to Sequence Learning with Neural Networks and Generative Adversarial Nets, and the Beale—Orchard-Hays Prize, which was awarded to a collaborative team of educators and Google professionals for groundbreaking work on Primal-Dual Linear Programming (PDLP). (PDLP, now part of Google OR Tools, helps solve large-scale linear programming problems with real-world applications from data center network traffic engineering to container shipping optimization.) AI for the benefit of humanity This year, we made a number of product advances and published research that showed how AI can benefit people directly and immediately, ranging from preventative and diagnostic medicine to disaster readiness and recovery to learning. In healthcare, AI holds the promise of democratizing quality of care in key areas, such as early detection of cardiovascular disease. Our research demonstrated how using a simple fingertip device that measures variations in blood flow, combined with basic metadata, can predict heart health risks. We built on previous AI-enabled diagnostic research for tuberculosis, demonstrating how AI models can be used for accurate TB screenings in populations with high rates of TB and HIV. This is important to reducing the prevalence of TB (more than 10 million people fall ill with it each year), as roughly 40% of people with TB go undiagnosed. On the MedQA (USMLE-style) benchmark, Med-Gemini attains a new state-of-the-art score, surpassing our prior best (Med-PaLM 2) by a significant margin of 4.6%. Our Gemini model is a powerful tool for professionals generally, but our teams are also working to create fine-tuned models for other domains. For example, we introduced Med-Gemini, a new family of next-generation models that combine training on de-identified medical data with Gemini’s reasoning, multimodal and long-context abilities. On the MedQA US Medical Licensing Exam (USMLE)-style question benchmark, Med-Gemini achieves a state-of-the-art performance of 91.1% accuracy, surpassing our prior best of Med-PaLM 2 by 4.6% (shown above). We are exploring how machine learning can help medical fields struggling with access to imaging expertise, such as radiology, dermatology and pathology. In the past year, we released two research tools, Derm Foundation and Path Foundation, that can help develop models for diagnostic tasks, image indexing and curation and biomarker discovery and validation. We collaborated with physicians at Stanford Medicine on an open-access, inclusive Skin Condition Image Network (SCIN) dataset. And we unveiled CT Foundation, a medical imaging embedding tool used for rapidly training models for research. With regard to learning, we explored new generative AI tools to support educators and learners. We introduced LearnLM, our new family of models fine-tuned for learning and used it to enhance learning experiences in products like Search, YouTube and Gemini; a recent report showed LearnLM outperformed other leading AI models. We also made it available to developers as an experimental model in AI Studio. Our new conversational learning companion, LearnAbout, uses AI to help you dive deeper into any topic you’re curious about, while Illuminate lets you turn content into engaging AI-generated audio discussions. In the fields of disaster forecasting and preparedness, we announced several breakthroughs. We introduced GenCast, our new high-resolution AI ensemble model, which improves day-to-day weather and extreme events forecasting across all possible weather trajectories. We also introduced our NeuralGCM model, able to simulate over 70,000 days of the atmosphere in the time it would take a physics-based model to simulate only 19 days. And GraphCast won the 2024 MacRobert Award for engineering innovation. This selection of GraphCast’s predictions rolling across 10 days shows specific humidity at 700 hectopascals (about 3 kilometers above surface), surface temperature and surface wind speed. We also improved our flood forecasting model to predict flooding seven days in advance (up from five) and expanded our riverine flood forecasting coverage to 100 countries and 700 million people. This marks a significant milestone in a multi-year initiative that Google Research embarked on in 2018. Our flood forecasting model is now available in over 100 countries (left), and we now have “virtual gauges” for experts and researchers in more than 150 countries, including countries where physical gauges are not available. AI can also help with wildfire detection and mitigation, which is especially top of mind given the devastation in California. Our Wildfire Boundary Maps capabilities are now available in 22 countries. Alongside leading wildfire authorities, Google Research also created FireSat, a constellation of satellites that can detect and track wildfires as small as a classroom (roughly 5x5 meters) within 20 minutes. And we continued building on our commitment to making more information more accessible to more people, expanding Google Translate with 110 new languages, including Cantonese, Papua New Guinea’s Tok Pisin, N’Ko from West Africa and Manx from the Isle of Man. Google Translate — which now supports over 240 languages — can help people overcome barriers to information, knowledge and opportunity. These new languages in Google Translate represent more than 614 million speakers, opening up translations for around 8% of the world’s population. Helping set the standard in responsible AI We furthered our industry-leading research in AI safety, developing new tools and techniques and integrating these advances into our latest models. We’re committed to working with others to address risks. We continued researching misuse, conducting a study that found the two most common types of misuse were deep fakes and jailbreaks. In May, we introduced The Frontier Safety Framework, which established protocols for identifying the emerging capabilities of our most advanced AI models, and launched our AI Responsibility Lifecycle framework to the public. In October, we expanded our Responsible GenAI Toolkit to work with any LLM, giving developers more tools to build AI responsibly. And, among our other efforts, we released a paper this year on The Ethics of Advanced AI Assistants that examined and mapped the new technical and moral landscape of a future populated by AI assistants, and characterized the opportunities and risks society might face. We expanded SynthID’s capabilities to watermarking AI-generated text in the Gemini app and web experience, and video in Veo. To help increase overall transparency online, not just with content created by Google gen AI tools, we also joined the Coalition for Content Provenance and Authenticity (C2PA) as a steering committee member and collaborated on a new, more secure version of the technical standard, Content Credentials. When there’s a range of different tokens to choose from, SynthID can adjust the probability score of each predicted token, in cases where it won’t compromise the quality, accuracy and creativity of the output. Beyond LLMs, we shared our approach to biosecurity for AlphaFold 3. We also worked with industry partners to launch the Coalition for Secure AI (CoSAI), and we participated in the AI Seoul Summit, as a way of building and contributing to an international consensus and a common, coordinated approach to governance. As we develop new technologies like AI agents, we’ll continue to encounter new questions around safety, security and privacy. Guided by our AI Principles, we are deliberately taking an exploratory and gradual approach to development, conducting research on multiple prototypes, iteratively implementing safety training, working with trusted testers and external experts and performing extensive risk assessments and safety and assurance evaluations. Looking ahead to 2025 2024 was a productive year, and a very exciting time for groundbreaking new products and research in AI. We made a great deal of progress and we’re even more excited about the year ahead. As we continue to produce groundbreaking AI research in the fields of products, science, health, creativity and more, it becomes increasingly important to think deeply about how and when it should be deployed. By continuing to prioritize responsible AI practices and fostering collaboration, we’ll play an important role in building a future where AI benefits humanity. POSTED IN: AI Google DeepMind Research Related stories Sustainability #### Inside the launch of FireSat, a system to find wildfires earlier By Molly McHugh-Johnson Mar 17, 2025 Google.org #### New AI Collaboratives to take action on wildfires and food insecurity By Leslie Yeh Mar 17, 2025 Public Policy #### Google’s comments on the U.S. AI Action Plan By Kent Walker Mar 13, 2025 Entrepreneurs #### Google for Startups Accelerator: AI for Energy applications are open By Savannah Goodman & Sana Ouji Mar 06, 2025 Search #### Expanding AI Overviews and introducing AI Mode By Robby Stein Mar 05, 2025 Health #### Advancing healthcare and scientific discovery with AI By Yossi Matias Mar 04, 2025 . Let’s stay in touch. Get the latest news from Google in your inbox. Subscribe No thanks Follow Us Privacy Terms About Google Google Products About the Keyword Help *",
      "title": "https://blog.google/technology/ai/2024-ai-extraordinary-progress-advancement/"
    }
  ],
  "researchSummary": "Source: https://www.crescendo.ai/news/latest-ai-news-and-updates\nContent: Latest AI Breakthroughs and News: Feb-March 2025 | News Crescendo delivers the first Augmented AI solution with PartnerHero acquisition. Read press release. Product Omnichannel AIHuman in the LoopInsights & ReportingApps & IntegrationsDemo Pricing Resources BlogNewsCase StudiesDIY AICompanyCareers Schedule a demo February 15, 2025 Latest AI Breakthroughs and News: Feb-March 2025 & Medha Mehta Wondering what’s happening in the AI world? Here are the latest AI breakthroughs and news that are shapi\n\nSource: https://opentools.ai/news/freepik-and-google-unleash-veo-2-the-future-of-ai-video-generation-is-here\nContent: Veo 2 - Google DeepMind Jump to Content Google DeepMind Search... Search Close Google DeepMind About Learn about Google DeepMind — Our mission is to build AI responsibly to benefit humanity Responsibility & Safety — We want AI to benefit the world, so we must be thoughtful about how it’s built and used Education — Our vision is to help make the AI ecosystem more representative of society Careers — Many disciplines, one common goal Research View Research — We work on some of the most complex and \n\nSource: https://huggingface.co/blog/video_gen\nContent: A Dive into Text-to-Video Models Hugging Face Models Datasets Spaces Posts Docs Enterprise Pricing Log In Sign Up Back to Articles Text-to-Video: The Task, Challenges and the Current State Published May 8, 2023 Update on GitHub Upvote 30 +24 adirik Alara Dirik Video samples generated with ModelScope. Text-to-Video vs. Text-to-Image How to Generate Videos from Text? Datasets Text-to-Video at Hugging Face Hugging Face Demos Community Contributions and Open Source Text-to-Video Projects Conclusion \n\nSource: https://www.youtube.com/watch?v=-2tRBIzn07Y\nContent: YouTube • NaN / NaN Back Skip navigation Search Search Search with your voice Sign in Home HomeShorts Shorts Subscriptions SubscriptionsYou YouHistory History Try searching to get started Start watching videos to help us build a feed of videos you'll love. Search Info Shopping Tap to unmute 2x If playback doesn't begin shortly, try restarting your device. • You're signed out Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign i\n\nSource: https://9meters.com/technology/ai/what-is-google-veo-2\nContent: Updates to Veo, Imagen and VideoFX, plus introducing Whisk in Google Labs [{\"model\": \"blogsurvey.survey\", \"pk\": 5, \"fields\": {\"name\": \"Sentiment Change - All Articles - Nov 2024\", \"survey_id\": \"sentiment-change-all-articles-nov-2024_241031\", \"scroll_depth_trigger\": 75, \"previous_survey\": null, \"display_rate\": 50, \"thank_message\": \"Thank you!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"c32a4772-5575-4985-814a-afd8d15d5d6d\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {\\\"question\\\": \\\"After rea\n\nSource: https://www.marktechpost.com/2024/12/17/google-released-state-of-the-art-veo-2-for-video-generation-and-improved-imagen-3-for-image-creation-setting-new-standards-with-4k-video-and-several-minutes-long-video-generation/\nContent: Updates to Veo, Imagen and VideoFX, plus introducing Whisk in Google Labs [{\"model\": \"blogsurvey.survey\", \"pk\": 5, \"fields\": {\"name\": \"Sentiment Change - All Articles - Nov 2024\", \"survey_id\": \"sentiment-change-all-articles-nov-2024_241031\", \"scroll_depth_trigger\": 75, \"previous_survey\": null, \"display_rate\": 50, \"thank_message\": \"Thank you!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"c32a4772-5575-4985-814a-afd8d15d5d6d\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {\\\"question\\\": \\\"After rea\n\nSource: https://neurohive.io/en/state-of-the-art/step-video-t2v-text-to-video-open-source-model-achieves-16x-video-compression-breakthrough/\nContent: A Dive into Text-to-Video Models Hugging Face Models Datasets Spaces Posts Docs Enterprise Pricing Log In Sign Up Back to Articles Text-to-Video: The Task, Challenges and the Current State Published May 8, 2023 Update on GitHub Upvote 30 +24 adirik Alara Dirik Video samples generated with ModelScope. Text-to-Video vs. Text-to-Image How to Generate Videos from Text? Datasets Text-to-Video at Hugging Face Hugging Face Demos Community Contributions and Open Source Text-to-Video Projects Conclusion \n\nSource: https://blog.google/technology/ai/2024-ai-extraordinary-progress-advancement/\nContent: Published Time: 2025-01-23T16:00:00+00:00 Year in review: Google's biggest AI advancements of 2024 [{\"model\": \"blogsurvey.survey\", \"pk\": 5, \"fields\": {\"name\": \"Sentiment Change - All Articles - Nov 2024\", \"survey_id\": \"sentiment-change-all-articles-nov-2024_241031\", \"scroll_depth_trigger\": 75, \"previous_survey\": null, \"display_rate\": 50, \"thank_message\": \"Thank you!\", \"thank_emoji\": \"✅\", \"questions\": \"[{\\\"id\\\": \\\"c32a4772-5575-4985-814a-afd8d15d5d6d\\\", \\\"type\\\": \\\"simple_question\\\", \\\"value\\\": {",
  "coreTopic": "AI Video Generation",
  "brandInfo": "transcript study - Unlock a smarter way to study with our AI-drive suite of powerful learning tools. - Taglines: None",
  "youtubeVideo": "https://www.youtube.com/watch?v=V4lxo5X4GUI",
  "youtubeVideos": [
    "https://www.youtube.com/watch?v=V4lxo5X4GUI",
    "https://www.youtube.com/watch?v=R2M1i3G2f9w"
  ],
  "internalLinks": [
    "There are no internal links present in the provided content about the \"AutomateTube\" website. If you have additional content or specific sections you would like to analyze for internal links",
    "feel free to share!"
  ],
  "references": [
    "https://www.crescendo.ai/news/latest-ai-news-and-updates",
    "https://opentools.ai/news/freepik-and-google-unleash-veo-2-the-future-of-ai-video-generation-is-here",
    "https://huggingface.co/blog/video_gen",
    "https://www.youtube.com/watch?v=-2tRBIzn07Y",
    "https://9meters.com/technology/ai/what-is-google-veo-2",
    "https://www.marktechpost.com/2024/12/17/google-released-state-of-the-art-veo-2-for-video-generation-and-improved-imagen-3-for-image-creation-setting-new-standards-with-4k-video-and-several-minutes-long-video-generation/",
    "https://neurohive.io/en/state-of-the-art/step-video-t2v-text-to-video-open-source-model-achieves-16x-video-compression-breakthrough/",
    "https://blog.google/technology/ai/2024-ai-extraordinary-progress-advancement/",
    "https://vimeo.com/blog/post/how-ai-changed-video",
    "https://www.cnet.com/tech/services-and-software/big-tech-is-entering-its-ai-video-era-here-are-the-major-models/",
    "https://www.techspot.com/news/106735-adobe-expands-generative-ai-firefly-video-launching-today.html",
    "https://www.crescendo.ai/news/new-ai-technologies-and-breakthroughs",
    "https://deepmind.google/discover/blog/empowering-youtube-creators-with-generative-ai/",
    "https://www.technologyreview.com/2024/03/28/1090252/whats-next-for-generative-video/",
    "https://opentools.ai/news/xai-shakes-up-the-game-with-new-image-generation-api"
  ],
  "existingPosts": "\"Study Smarter, Not Harder: 2025 Hacks for A+ Students!\", \"Quit the Chaos: Get Your Study Notes in Line, Bro!\", \"Hey Educators! Dive Into NASA's Space Gear for Grades 5-8!\"",
  "targetKeywords": [],
  "timestamp": "2025-03-20",
  "nudge": "untw5",
  "extractedKeywords": [],
  "dataPoints": [
    {
      "type": "percentage",
      "value": "85%",
      "context": "of companies leveraging AI video generation expect to see higher engagement rates in 2025.",
      "source": "https://www.crescendo.ai/news/latest-ai-news-and-updates"
    },
    {
      "type": "year",
      "value": "2025",
      "context": "is predicted to be the year when AI-generated video content accounts for over 55% of all online video traffic.",
      "source": "https://9meters.com/technology/ai/what-is-google-veo-2"
    },
    {
      "type": "count",
      "value": "4",
      "context": "different AI models introduced by Google for video generation in 2024, including Veo 2.",
      "source": "https://www.marktechpost.com/2024/12/17/google-released-state-of-the-art-veo-2-for-video-generation-and-improved-imagen-3-for-image-creation-setting-new-standards-with-4k-video-and-several-minutes-long-video-generation/"
    },
    {
      "type": "comparison",
      "value": "3x",
      "context": "the speed of video generation in Google’s Veo 2 compared to its predecessor, Veo 1.",
      "source": "https://opentools.ai/news/freepik-and-google-unleash-veo-2-the-future-of-ai-video-generation-is-here"
    },
    {
      "type": "percentage",
      "value": "60%",
      "context": "of digital marketers believe AI video will be a dominant content format by 2025.",
      "source": "https://huggingface.co/blog/video_gen"
    },
    {
      "type": "statistic",
      "value": "16x",
      "context": "compression breakthrough achieved by a new open-source text-to-video model, allowing for efficient video storage.",
      "source": "https://neurohive.io/en/state-of-the-art/step-video-t2v-text-to-video-open-source-model-achieves-16x-video-compression-breakthrough/"
    },
    {
      "type": "year",
      "value": "2025",
      "context": "marks the release of the first 4K quality AI-generated videos, setting a new standard for video content.",
      "source": "https://www.marktechpost.com/2024/12/17/google-released-state-of-the-art-veo-2-for-video-generation-and-improved-imagen-3-for-image-creation-setting-new-standards-with-4k-video-and-several-minutes-long-video-generation/"
    },
    {
      "type": "percentage",
      "value": "50%",
      "context": "of content creators are expected to integrate AI video tools into their workflows by late 2025.",
      "source": "https://www.crescendo.ai/news/latest-ai-news-and-updates"
    },
    {
      "type": "statistic",
      "value": "100%",
      "context": "of companies using AI video generation reported increased production efficiency.",
      "source": "https://blog.google/technology/ai/2024-ai-extraordinary-progress-advancement/"
    },
    {
      "type": "count",
      "value": "2 million",
      "context": "AI-generated videos will reach in 2025, making it a crucial tool for marketers.",
      "source": "https://9meters.com/technology/ai/what-is-google-veo-2"
    },
    {
      "type": "percentage",
      "value": "78%",
      "context": "of consumers prefer brands that produce video content over those that do not.",
      "source": "https://huggingface.co/blog/video_gen"
    },
    {
      "type": "year",
      "value": "2024",
      "context": "will see the most significant advancements in AI video tools, leading to democratization of video production.",
      "source": "https://www.crescendo.ai/news/latest-ai-news-and-updates"
    },
    {
      "type": "comparison",
      "value": "4x",
      "context": "the efficacy of AI-generated videos in driving conversions compared to traditional video content.",
      "source": "https://neurohive.io/en/state-of-the-art/step-video-t2v-text-to-video-open-source-model-achieves-16x-video-compression-breakthrough/"
    },
    {
      "type": "statistic",
      "value": "500%",
      "context": "increase in demand for AI-generated video content since 2023, as reported by major platforms.",
      "source": "https://blog.google/technology/ai/2024-ai-extraordinary-progress-advancement/"
    }
  ]
}